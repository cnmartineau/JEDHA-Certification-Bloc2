{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bloc 2 - Analyse exploratoire, descriptive et inférentielle de données - Speed dating with Tinder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Tinder is an online dating and geosocial networking application launched in 2012. It is the world’s most popular application for meeting new people. The application is available in over 190 countries and 56 languages. It has been downloaded more than 530 million times and led to more than 75 billion matches. In 2023, Tinder had 10.9 million subscribers and 75 million monthly active users. \n",
    "\n",
    "Tinder matches users based on geographic proximity. In Tinder, users \"swipe right\" to like or \"swipe left\" to dislike other users' profiles, which include their photos, a short bio, and a list of their interests. Tinder uses a \"double opt-in\" system where both users must like each other (match) before they can exchange messages.\n",
    "\n",
    "### Problematic\n",
    "\n",
    "In 2002, Tinder experienced a decrease in the number of matches.\n",
    "\n",
    "The company would like to understand the unlerlying basis of this drop.\n",
    "\n",
    "### Scope\n",
    "\n",
    "To better understand this decline in matches, the Tinder marketing team decided to focus on the profile of potential users in order to identify the key features that influence matching.\n",
    "\n",
    "They ran a speed dating experiment, split into 21 experimental speed dating events (waves) from 2002 to 2004. During the events, attendees would have a four minute \"first date\" with every other participant of the opposite sex. At the end of their dates, participants were asked if they would like to see the other persons again.\n",
    "\n",
    "Participants would have to give Tinder extensive information about themselves, that could ultimately reflect on their dating profile on the application. Information was collected in several surveys before, during, and after the event. The dataset includes information on each speed date, as well as information on demographics, dating habits, lifestyle information, self-perception across key attributes, and beliefs on what others find valuable in a mate.\n",
    "\n",
    "### Aim and objectives\n",
    "\n",
    "Overall aim: Understand what makes people interested into each other.\n",
    "\n",
    "Objectives:\n",
    "- 1 - Evaluate the importance and impact of partner's attributes in decision making.\n",
    "- 2 - Evaluate the impact of shared interests between partners.\n",
    "- 3 - Evaluate the importance and impact of shared ethnicity between patners.\n",
    "- 4 - Evaluate the importance and impact of self-esteem for obtaining a like and a match.\n",
    "- 5 - Evaluate the importance and impact of shared dating goals between partners."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##\n",
    "## Methods\n",
    "\n",
    "### 1 - Library import\n",
    "\n",
    "### 2 - File reading and basic exploration\n",
    "\n",
    "The dataset was composed of records of various information on 8378 speed dates performed in 21 waves with 551 individual subjects. It contained 195 features.\n",
    "\n",
    "The initial inspection of the dataset revealed that the data key was not always reliable, that categorical features were encoded, that the dataset contained many missing values, some outliers, some values out of the given scale of the features, and some numeric values stored as strings. Moreover, different scales were used between waves for the quotation of some attributes.\n",
    "\n",
    "Therefore, the dataset needed preprocessing to make it suitable for further analysis.\n",
    "\n",
    "### 3 - Preprocessing\n",
    "\n",
    "According to what was observed, and according to the structure of the dataset, a first preprocessing was performed with the following steps:\n",
    "- 1 - Assessment of discrepencies within each subject-related feature, per subject.\n",
    "- 2 - Corrections on discrepencies in id and positin1 features.\n",
    "- 3 - Renaming of the content of categorical features (according to the data key, when possible).\n",
    "- 4 - Ad hoc corrections on numeric features stored as strings.\n",
    "- 5 - Assessment of the quality of the survey quotations and ad hoc corrections, rescaling.\n",
    "- 6 - Correction of values that were rescaled.\n",
    "- 7 - Indentification and treatment of outliers\n",
    "\n",
    "Since the dataset covered dates between 551 subjects within 21 waves, it contained a lot of redundant information (speed dates are reciprocal). Moreover, not all subjects had the same number of speed dates,\n",
    "thus the dataset was biased towards subjects that had more dates. For these reasons, the dataset has been reduced to obtain a second dataset with data grouped by subject to plot demographics info.\n",
    "\n",
    "### 4 - Overview 1 - Quality of the speed dating experiment\n",
    "\n",
    "The dataset contained recordings of information given by the subjects at several time points of the experiment. Before analysing the data, the quality of the experiment was assessed. \n",
    "\n",
    "The quality of surveys was assessed by examining the proportion of fully and partially filled surveys (some general info on subjects may be missing, some subjects may not have filled all the surveys). The effectiveness of the dating process  was assessed by examining the proportion of likes, matches, and resulting actual dates. The numbers of likes and matches relative to the order of the speed dates were graphically represented.\n",
    "\n",
    "### 5 - Overview 2 - Demographics of the cohort\n",
    "\n",
    "The original dataset being biased towards subjects that had more dates than others, the reduced data was used to plot demographics of the cohort (i.e. not demographics of the experiment itself).\n",
    "\n",
    "### 6 - Analysis 1 - Importance and impact of partner's attributes\n",
    "\n",
    "Subjects were asked to rate a series of attributes that they desire to find in a partner for a successful date. The importance of attributes was compared between males and females to identify partner expectations before a speed date.\n",
    "\n",
    "For females, intelligence seemed to be the most desired attribute, while it seemed to be attractiveness for males. This result was compared to the real impact of these attributes on giving a like to the partner.\n",
    "\n",
    "### 7 - Analysis 2 - Impact of shared interests\n",
    "\n",
    "Dating apps often ask for people their interest in given activities, not only for people to get to know each other, but also as a mean to engage conversation when a match occurs. However, the impact of shared interests for getting a match was not proven.\n",
    "\n",
    "Shared interests were the second least desired attribute when meeting a potential partner, both for females and males. To assess whether they had a real impact on people to give a like after a speed date, the Euclidean distance between partners (relative to activities) was calculated and the two groups (no like versus like) were compared, for females and for males. The impact on getting a match was also assessed.\n",
    "\n",
    "### 8 - Analysis 3 - Importance and impact of shared ethnicity\n",
    "\n",
    "Shared ethnicities might be important for giving a like and a match and might differ amongst ethnicities. Preferences were compared amongst ethnicity groups. The impact of shared ethnicity on giving a like and getting a match was displayed for each ethnicity.\n",
    "\n",
    "### 9 - Analysis 4 - Importance and impact of self-esteem\n",
    "\n",
    "Self-esteem is defined as the ability of the subject to grade its own attributes at least as well as the grades given by the speed date partner for the same attributes. It was measured as the mean of the differences between these two grades.\n",
    "\n",
    "The measure of self-esteem was compared between females and males. Its impact on getting a like from the partner and on getting a match was assessed.\n",
    "\n",
    "### 10 - Analysis 5 - Importance and impact of the dating goal\n",
    "\n",
    "Having shared goals might be of crucial importance to find the right partner. Answers given by the subjects about their primary goal in dating were compared between females and males. The number of likes given for each category was also compared between genders, as well as the proportion of matches per category.\n",
    "\n",
    "The impact of having shared goals was assessed by calculating the proportion of likes given to a partner of the same category over the total number of likes given by that category. The same logic was followed to assess the impact of shared goals on matches."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##\n",
    "## Conclusion\n",
    "\n",
    "The experiment carried out by Tinder was quite comprehensive, and many hypotheses which would explain the decrease in the number of matches could have been tested. To better understand why people match or not, this project gave insights about the importance and impact on matching for several key attributes of the speed dating partners, including their background, qualities, hobbies, self-esteem, and dating goals.\n",
    "\n",
    "The results of this project showed that matches are influenced by multiple factors.\n",
    "- Regarding the background, it appeared that some ethnicities, like caucasians, are more prone than others to match with people of the same ethnicity. \n",
    "- Regarding qualities, women statistically gave more likes to men that they qualified as intelligent, while men statistically gave more likes to women that they qualified as attractive.\n",
    "- Regarding shared interests, the divergence of interests between speed dating partners did not explain a difference in the number of matches.\n",
    "- Regarding self-esteem, people under-estimating their qualities (in comparison to the grades given by their partners) had statistically less chances to receive a like or to obtain a match.\n",
    "- Regarding dating goals, it appeared that people having a light goal tended to better match together.\n",
    "\n",
    "To improve the number of matches, it might be useful for Tinder to optimise the information they first display on the screen, especially that people do not necessarily read the entire profile of their potential future date. For example, replacing the shared hobbies by the shared goals could influence users on reading further, or at least help them in making quick decisions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##\n",
    "## Code"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Library import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1 - library import ### ----\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import scipy.stats as stats\n",
    "import scipy.spatial as spatial\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\n",
    "### 2 - File reading and basic exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2 - file reading and basic exploration - import dataset ### ----\n",
    "\n",
    "# read data\n",
    "data = pd.read_csv(\"cnm_bloc2_data.csv\", encoding = \"ISO-8859-1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2 - file reading and basic exploration - get basic stats ### ----\n",
    "\n",
    "# print shape of data\n",
    "print(\"Number of rows: {}\".format(data.shape[0]))\n",
    "print(\"Number of columns: {}\".format(data.shape[1]))\n",
    "print()\n",
    "\n",
    "# display dataset\n",
    "pd.set_option('display.max_columns', None)\n",
    "print(\"Dataset display: \")\n",
    "display(data.head())\n",
    "print()\n",
    "\n",
    "# display basic statistics\n",
    "print(\"Basics statistics: \")\n",
    "data_desc = data.describe(include='all')\n",
    "display(data_desc)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2 - file reading and basic exploration - get percentage of missing values ### ----\n",
    "\n",
    "# check wether some columns are full of NaNs\n",
    "column_nan_full = data.columns[data.isnull().all()]\n",
    "column_nb = len(column_nan_full)\n",
    "\n",
    "# get percent of NaNs per column to output overview\n",
    "column_nan_percent = data.isnull().sum() * 100 /data.shape[0]\n",
    "column_nan_distrib = pd.Series(np.zeros(4),index = [\"less than 10%\",\"10% to 50%\",\"50% to 70%\",\"70% and more\"])\n",
    "column_nan_distrib[0] = len(column_nan_percent[column_nan_percent < 10])\n",
    "column_nan_distrib[1] = len(column_nan_percent[(column_nan_percent >= 10) & (column_nan_percent < 50)])\n",
    "column_nan_distrib[2] = len(column_nan_percent[(column_nan_percent >= 50) & (column_nan_percent < 70)])\n",
    "column_nan_distrib[3] = len(column_nan_percent[column_nan_percent >= 70])\n",
    "\n",
    "# check wether some rows are full of NaNs\n",
    "row_nan_count = pd.Series([data.loc[i,:].isnull().sum() for i in range(0, data.shape[0])])\n",
    "row_nan_full = row_nan_count.index[row_nan_count == data.shape[1]]\n",
    "row_nb = len(row_nan_full)\n",
    "\n",
    "# get percent of NaNs per row to output overview\n",
    "row_nan_percent = row_nan_count * 100 / data.shape[1]\n",
    "row_nan_distrib = pd.Series(np.zeros(4),index = [\"less than 10%\",\"10% to 50%\",\"50% to 70%\",\"70% and more\"])\n",
    "row_nan_distrib[0] = len(row_nan_percent[row_nan_percent < 10])\n",
    "row_nan_distrib[1] = len(row_nan_percent[(row_nan_percent >= 10) & (row_nan_percent < 50)])\n",
    "row_nan_distrib[2] = len(row_nan_percent[(row_nan_percent >= 50) & (row_nan_percent < 70)])\n",
    "row_nan_distrib[3] = len(row_nan_percent[row_nan_percent >= 70])\n",
    "\n",
    "# print report\n",
    "print(\"COLUMNS\")\n",
    "print(\"{} columns out of {} are fully filled with missing values\".format(column_nb,data.shape[1]))\n",
    "print(\"Percentage of missing values (number of columns):\\n{}\".format(column_nan_distrib) + \"\\n\")\n",
    "print(\"ROWS\")\n",
    "print(\"{} rows out of {} are fully filled with missing values\".format(row_nb,data.shape[0]))\n",
    "print(\"Percentage of missing values (number of rows):\\n{}\".format(row_nan_distrib))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\n",
    "### 3 - Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3 - preprocessing - assess discrepencies within each subject-related feature by subject ### ----\n",
    "\n",
    "# some features should be unique within each subject\n",
    "# since the dataset is slightly funky:\n",
    "# - look for the mixed presence of values and NaNs within these features\n",
    "# - look for multiple unique values within these features\n",
    "\n",
    "# identify columns that are not supposed to contain unique data\n",
    "columns_drop = ['iid', 'position', 'order', 'partner', 'pid', 'match', 'int_corr', 'samerace',\n",
    "       'age_o', 'race_o', 'pf_o_att', 'pf_o_sin', 'pf_o_int', 'pf_o_fun','pf_o_amb',\n",
    "       'pf_o_sha', 'dec_o', 'attr_o', 'sinc_o', 'intel_o', 'fun_o', 'amb_o', 'shar_o',\n",
    "       'like_o', 'prob_o', 'met_o', 'dec', 'attr', 'sinc', 'intel', 'fun', 'amb', 'shar',\n",
    "       'like', 'prob', 'met', 'match_es']\n",
    "columns_tocheck = data.columns.drop(columns_drop)\n",
    "\n",
    "# look for discrepencies within each info per subject\n",
    "\n",
    "# get subjects id and number of subjects\n",
    "subjects_id = data[\"iid\"].unique()\n",
    "subjects_nb = len(subjects_id)\n",
    "\n",
    "# initialise variables to store errors\n",
    "errors_nan = pd.DataFrame(index = subjects_id, columns = columns_tocheck)\n",
    "errors_unique = pd.DataFrame(index = subjects_id, columns = columns_tocheck)\n",
    "\n",
    "# loop through subjects\n",
    "for i in subjects_id:\n",
    "\n",
    "    # loop through features\n",
    "    for j in columns_tocheck:\n",
    "\n",
    "       # get data\n",
    "       data_current = data.loc[data[\"iid\"] == i,j]\n",
    "\n",
    "       # check for the presence of partially filled info\n",
    "       if (data_current.notnull().any()) & (data_current.isnull().any()):\n",
    "              errors_nan.loc[i,j] = 1\n",
    "\n",
    "       # get unique values per info (without missing values)\n",
    "       data_current = data_current[data_current.notnull()]\n",
    "       values_unique = data_current.unique()\n",
    "\n",
    "       # store error if there is more than one unique value per info\n",
    "       if len(values_unique) > 1:\n",
    "              errors_unique.loc[i,j] = 1\n",
    "\n",
    "# get errors summary and print result\n",
    "errors_nan_summ = errors_nan.sum()\n",
    "if errors_nan_summ.isnull().all():\n",
    "       print(\"No features are partially filled\")\n",
    "else:\n",
    "       errors_nan_summ = errors_nan_summ.index[errors_nan_summ > 0]\n",
    "       print(\"These features are partially filled:\\n{}\\n\".format(errors_nan_summ))\n",
    "errors_unique_summ = errors_unique.sum()\n",
    "if errors_unique_summ.isnull().all():\n",
    "       print(\"No features show discrepencies in values\")\n",
    "else:\n",
    "       errors_unique_summ = errors_unique_summ.index[errors_unique_summ > 0]\n",
    "       print(\"These features show discrepencies in values:\\n{}\\n\".format(errors_unique_summ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3 - preprocessing - apply corrections on discrepencies in id and positin1 ### ----\n",
    "\n",
    "# errors were found in id and positin1 features\n",
    "# according to the data key, these values should be unique per subject\n",
    "\n",
    "# id records the subject number within the wave\n",
    "# - identify subjects for whom this mistake occurs\n",
    "# - fill NaNs with the id found for the subject in other speed dates\n",
    "\n",
    "# positin1 records the station number of the first date\n",
    "# since there is no way to infer the positin1 value from the data:\n",
    "# - identify subjects for whom this mistake occurs\n",
    "# - replace positin1 values by NaN for theses subjects\n",
    "\n",
    "# copy data for safety\n",
    "data1 = data.copy()\n",
    "\n",
    "# identify subjects\n",
    "subjects_nan = errors_nan.index[errors_nan[\"id\"] == 1]\n",
    "subjects_unique = errors_unique.index[errors_unique[\"positin1\"] == 1]\n",
    "\n",
    "# apply corrections\n",
    "for i in subjects_nan:\n",
    "    data1.loc[data1[\"iid\"] == i,\"id\"] = data1.loc[data1[\"iid\"] == i,\"id\"].min()\n",
    "for i in subjects_unique:\n",
    "    data1.loc[data1[\"iid\"] == i,\"positin1\"] = np.nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3 - preprocessing - rename the content of categorical features ### ----\n",
    "\n",
    "# copy data for safety\n",
    "data2 = data1.copy()\n",
    "\n",
    "# gender\n",
    "data2[\"gender\"] = data2[\"gender\"].apply(lambda x: \"Female\" if x == 0\n",
    "                                                else \"Male\" if x == 1\n",
    "                                                else np.nan)\n",
    "\n",
    "# condtn\n",
    "data2[\"condtn\"] = data2[\"condtn\"].apply(lambda x: \"Limited<br>choice\" if x == 1\n",
    "                                                else \"Extensive<br>choice\" if x == 2\n",
    "                                                else np.nan)\n",
    "\n",
    "# match\n",
    "data2[\"match\"] = data2[\"match\"].apply(lambda x: \"No\" if x == 0\n",
    "                                                else \"Yes\" if x == 1\n",
    "                                                else np.nan)\n",
    "\n",
    "# samerace\n",
    "data2[\"samerace\"] = data2[\"samerace\"].apply(lambda x: \"No\" if x == 0\n",
    "                                                else \"Yes\" if x == 1\n",
    "                                                else np.nan)\n",
    "# race_o\n",
    "data2[\"race_o\"] = data2[\"race_o\"].apply(lambda x: \"African\" if x == 1\n",
    "                                                else \"Caucasian\" if x ==2\n",
    "                                                else \"Latino\" if x ==3\n",
    "                                                else \"Asian\" if x == 4\n",
    "                                                else \"Native\" if x == 5\n",
    "                                                else \"Other\" if x == 6\n",
    "                                                else np.nan)\n",
    "\n",
    "# dec_o\n",
    "data2[\"dec_o\"] = data2[\"dec_o\"].apply(lambda x: \"No\" if x == 0\n",
    "                                                else \"Yes\" if x == 1\n",
    "                                                else np.nan)\n",
    "\n",
    "# WARNING: problem in met_o > unique values range from 0 to 8\n",
    "# data2[\"met_o\"] = data2[\"met_o\"].apply(lambda x: \"Yes\" if x == 1\n",
    "                                                #else \"No\" if x == 2\n",
    "                                                #else np.nan)\n",
    "\n",
    "# field_cd\n",
    "data2[\"field_cd\"] = data2[\"field_cd\"].apply(lambda x: \"Law\" if x == 1\n",
    "                                                else \"Math\" if x ==2\n",
    "                                                else \"Social Science<br>Psychologist\" if x ==3\n",
    "                                                else \"Medical Science<br>Pharmaceuticals<br>Bio Tech\" if x == 4\n",
    "                                                else \"Engineering\" if x == 5\n",
    "                                                else \"English<br>Creative Writing<br>Journalism\" if x == 6\n",
    "                                                else \"History<br>Religion<br>Philosophy\" if x == 7\n",
    "                                                else \"Business<br>Economy<br>Finance\" if x == 8\n",
    "                                                else \"Education<br>Academia\" if x == 9\n",
    "                                                else \"Biological Sciences<br>Chemistry<br>Physics\" if x == 10\n",
    "                                                else \"Social Work\" if x == 11\n",
    "                                                else \"Undergraduate<br>Undecided\" if x == 12\n",
    "                                                else \"Political Science<br>International Affairs\" if x == 13\n",
    "                                                else \"Film\" if x == 14\n",
    "                                                else \"Fine Arts<br>Arts Administration\" if x == 15\n",
    "                                                else \"Languages\" if x == 16\n",
    "                                                else \"Architecture\" if x == 17\n",
    "                                                else \"Other\" if x == 18\n",
    "                                                else np.nan)\n",
    "\n",
    "# race\n",
    "data2[\"race\"] = data2[\"race\"].apply(lambda x: \"African\" if x == 1\n",
    "                                                else \"Caucasian\" if x ==2\n",
    "                                                else \"Latino\" if x ==3\n",
    "                                                else \"Asian\" if x == 4\n",
    "                                                else \"Native\" if x == 5\n",
    "                                                else \"Other\" if x == 6\n",
    "                                                else np.nan)\n",
    "\n",
    "# goal\n",
    "data2[\"goal\"] = data2[\"goal\"].apply(lambda x: \"Seemed like a<br>fun night out\" if x == 1\n",
    "                                                else \"To meet<br>new people\" if x ==2\n",
    "                                                else \"To get<br>a date\" if x ==3\n",
    "                                                else \"Looking for<br>a serious<br>relationship\" if x == 4\n",
    "                                                else \"To say<br>I did it\" if x == 5\n",
    "                                                else \"Other\" if x == 6\n",
    "                                                else np.nan)\n",
    "\n",
    "# date\n",
    "data2[\"date\"] = data2[\"date\"].apply(lambda x: \"Several times<br>a week\" if x == 1\n",
    "                                                else \"Twice<br>a week\" if x ==2\n",
    "                                                else \"Once<br>a week\" if x ==3\n",
    "                                                else \"Twice<br>a month\" if x == 4\n",
    "                                                else \"Once<br>a month\" if x == 5\n",
    "                                                else \"Several times<br>a year\" if x == 6\n",
    "                                                else \"Almost<br>never\" if x == 7\n",
    "                                                else np.nan)\n",
    "\n",
    "# go_out\n",
    "data2[\"go_out\"] = data2[\"go_out\"].apply(lambda x: \"Several times<br>a week\" if x == 1\n",
    "                                                else \"Twice<br>a week\" if x ==2\n",
    "                                                else \"Once<br>a week\" if x ==3\n",
    "                                                else \"Twice<br>a month\" if x == 4\n",
    "                                                else \"Once<br>a month\" if x == 5\n",
    "                                                else \"Several times<br>a year\" if x == 6\n",
    "                                                else \"Almost<br>never\" if x == 7\n",
    "                                                else np.nan)\n",
    "\n",
    "# career_c\n",
    "data2[\"career_c\"] = data2[\"career_c\"].apply(lambda x: \"Lawyer\" if x == 1\n",
    "                                                else \"Academic - Research\" if x ==2\n",
    "                                                else \"Psychologist\" if x ==3\n",
    "                                                else \"Doctor - Medicine\" if x == 4\n",
    "                                                else \"Engineer\" if x == 5\n",
    "                                                else \"Creative Arts - Entertainment\" if x == 6\n",
    "                                                else \"Banking - Consulting - Finance - Marketing - Business - CEO - Entrepreneur - Admin\" if x == 7\n",
    "                                                else \"Real Estate\" if x == 8\n",
    "                                                else \"International - Humanitarian Affairs\" if x == 9\n",
    "                                                else \"Undecided\" if x == 10\n",
    "                                                else \"Social Work\" if x == 11\n",
    "                                                else \"Speech Pathology\" if x == 12\n",
    "                                                else \"Politics\" if x == 13\n",
    "                                                else \"Pro sports - Athletics\" if x == 14\n",
    "                                                else \"Other\" if x == 15\n",
    "                                                else \"Journalism\" if x == 16\n",
    "                                                else \"Architecture\" if x == 17\n",
    "                                                else np.nan)\n",
    "\n",
    "# dec\n",
    "data2[\"dec\"] = data2[\"dec\"].apply(lambda x: \"No\" if x == 0\n",
    "                                                else \"Yes\" if x == 1\n",
    "                                                else np.nan)\n",
    "\n",
    "# WARNING: problem in met > unique values range from 0 to 8\n",
    "# data2[\"met\"] = data2[\"met\"].apply(lambda x: \"Yes\" if x == 1\n",
    "                                                #else \"No\" if x == 2\n",
    "                                                #else np.nan)\n",
    "\n",
    "# length\n",
    "data2[\"length\"] = data2[\"length\"].apply(lambda x: \"Too little\" if x == 1\n",
    "                                                else \"Too much\" if x == 2\n",
    "                                                else \"Just Right\" if x == 3\n",
    "                                                else np.nan)\n",
    "\n",
    "# numdat_2\n",
    "data2[\"numdat_2\"] = data2[\"numdat_2\"].apply(lambda x: \"Too few\" if x == 1\n",
    "                                                else \"Too many\" if x == 2\n",
    "                                                else \"Just Right\" if x == 3\n",
    "                                                else np.nan)\n",
    "\n",
    "# date_3\n",
    "data2[\"date_3\"] = data2[\"date_3\"].apply(lambda x: \"No\" if x == 0\n",
    "                                                else \"Yes\" if x == 1\n",
    "                                                else np.nan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3 preprocessing - apply ad hoc corrections on numeric features stored as strings ### ----\n",
    "\n",
    "# copy data for safety\n",
    "data3 = data2.copy()\n",
    "\n",
    "# identify categorical features\n",
    "columns_cat = data3.columns.drop(data2._get_numeric_data().columns)\n",
    "\n",
    "# identify numeric variables stored as strings (by eye)\n",
    "columns_cat_types = data3.loc[:,columns_cat].dtypes\n",
    "print(\"Check for supposed numeric variables within this list:\\n{}\".format(columns_cat_types))\n",
    "\n",
    "# mn_sat, tuition, and income were stored as strings\n",
    "# replace the \",\" by \"\" for numbers to be recognised\n",
    "for i in range(0,data3.shape[0]):\n",
    "    if type(data3.loc[i,\"mn_sat\"]) == str:\n",
    "        data3.loc[i,\"mn_sat\"] = data3.loc[i,\"mn_sat\"].replace(\",\",\"\")\n",
    "    if type(data3.loc[i,\"tuition\"]) == str:\n",
    "        data3.loc[i,\"tuition\"] = data3.loc[i,\"tuition\"].replace(\",\",\"\")\n",
    "    if type(data3.loc[i,\"income\"]) == str:\n",
    "        data3.loc[i,\"income\"] = data3.loc[i,\"income\"].replace(\",\",\"\")\n",
    "\n",
    "# update type of columns\n",
    "data3[\"mn_sat\"] = data3[\"mn_sat\"].astype(float)\n",
    "data3[\"tuition\"] = data3[\"tuition\"].astype(float)\n",
    "data3[\"income\"] = data3[\"income\"].astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3 preprocessing - assess quality of the survey quotations and apply corrections ### ----\n",
    "\n",
    "# some funky values were found when visually inspecting the dataset:\n",
    "# values out of the scale given by the data key document, which is totally misleading\n",
    "\n",
    "# - reassign features to the right scale after checking\n",
    "# - correct min and max values according to scale\n",
    "# - for quotation series, rescale all series with scale 1-10 to shared scale 0-100\n",
    "\n",
    "# copy data for safety\n",
    "data4 = data3.copy()\n",
    "\n",
    "# get numeric features that belong to a 1-10 quotation system (after checking myself)\n",
    "columns_scale10 = [\"imprace\",\"imprelig\",\"sports\",\"tvsports\",\"exercise\",\"dining\",\"museums\",\"art\",\"hiking\",\n",
    "    \"gaming\",\"clubbing\",\"reading\",\"tv\",\"theater\",\"movies\",\"concerts\",\"music\",\"shopping\",\"yoga\",\"exphappy\",\n",
    "    \"like\",\"prob\",\"like_o\",\"prob_o\",\"satis_2\",\n",
    "    \"attr3_1\",\"sinc3_1\",\"fun3_1\",\"intel3_1\",\"amb3_1\",\n",
    "    \"attr5_1\",\"sinc5_1\",\"intel5_1\",\"fun5_1\",\"amb5_1\",\n",
    "    \"attr\",\"sinc\",\"intel\",\"fun\",\"amb\",\"shar\",\n",
    "    \"attr3_s\",\"sinc3_s\",\"intel3_s\",\"fun3_s\",\"amb3_s\",\n",
    "    \"attr3_2\",\"sinc3_2\",\"intel3_2\",\"fun3_2\",\"amb3_2\",\n",
    "    \"attr5_2\",\"sinc5_2\",\"intel5_2\",\"fun5_2\",\"amb5_2\",\n",
    "    \"attr3_3\",\"sinc3_3\",\"intel3_3\",\"fun3_3\",\"amb3_3\",\n",
    "    \"attr5_3\",\"sinc5_3\",\"intel5_3\",\"fun5_3\",\"amb5_3\",\n",
    "    \"attr_o\",\"sinc_o\",\"intel_o\",\"fun_o\",\"amb_o\",\"shar_o\"]\n",
    "\n",
    "# get numeric features that belong to a mixed or a shared 0-100 quotation system (after checking myself)\n",
    "serie1 = [\"attr1_1\",\"sinc1_1\",\"intel1_1\",\"fun1_1\",\"amb1_1\",\"shar1_1\"]\n",
    "serie2 = [\"attr2_1\",\"sinc2_1\",\"intel2_1\",\"fun2_1\",\"amb2_1\",\"shar2_1\"]\n",
    "serie3 = [\"attr1_2\",\"sinc1_2\",\"intel1_2\",\"fun1_2\",\"amb1_2\",\"shar1_2\"]\n",
    "serie4 = [\"attr2_2\",\"sinc2_2\",\"intel2_2\",\"fun2_2\",\"amb2_2\",\"shar2_2\"]\n",
    "serie5 = [\"attr1_3\",\"sinc1_3\",\"intel1_3\",\"fun1_3\",\"amb1_3\",\"shar1_3\"]\n",
    "serie6 = [\"pf_o_att\",\"pf_o_sin\",\"pf_o_int\",\"pf_o_fun\",\"pf_o_amb\",\"pf_o_sha\"]\n",
    "serie7 = [\"attr1_s\",\"sinc1_s\",\"intel1_s\",\"fun1_s\",\"amb1_s\",\"shar1_s\"]\n",
    "serie8 =[\"attr4_1\",\"sinc4_1\",\"intel4_1\",\"fun4_1\",\"amb4_1\",\"shar4_1\"]\n",
    "serie9 = [\"attr4_2\",\"sinc4_2\",\"intel4_2\",\"fun4_2\",\"amb4_2\",\"shar4_2\"]\n",
    "serie10 = [\"attr7_2\",\"sinc7_2\",\"intel7_2\",\"fun7_2\",\"amb7_2\",\"shar7_2\"]\n",
    "serie11 = [\"attr4_3\",\"sinc4_3\",\"intel4_3\",\"fun4_3\",\"amb4_3\",\"shar4_3\"]\n",
    "serie12 = [\"attr2_3\",\"sinc2_3\",\"intel2_3\",\"fun2_3\",\"amb2_3\",\"shar2_3\"]\n",
    "serie13 = [\"attr7_3\",\"sinc7_3\",\"intel7_3\",\"fun7_3\",\"amb7_3\",\"shar7_3\"]\n",
    "\n",
    "# assemble feature lists\n",
    "series_list = [serie1, serie2, serie3, serie4, serie5, serie6, serie7, serie8, serie9, serie10,serie11, \n",
    "    serie12, serie13]\n",
    "series_all = pd.DataFrame(series_list, columns = range(0,6))\n",
    "\n",
    "# check features of scale 1-10\n",
    "for i in columns_scale10:\n",
    "    \n",
    "    # get data\n",
    "    data_current = data4[i]\n",
    "\n",
    "    # apply correction to minimum and maximum values\n",
    "    if data_current.min() < 1:\n",
    "        data4.loc[data4[i] < 1,i] = 1\n",
    "    if data_current.max() > 10:\n",
    "        data4.loc[data4[i] > 10,i] = 10\n",
    "\n",
    "# check features with mixed scales\n",
    "# loop through series\n",
    "for i in range(0,series_all.shape[0]):\n",
    "\n",
    "    # get columns\n",
    "    columns_current = series_all.loc[i,series_all.loc[i,:].notnull()]\n",
    "\n",
    "    # loop through rows\n",
    "    for j in range(0, data4.shape[0]):\n",
    "\n",
    "        # get data\n",
    "        data_current = data4.loc[j,columns_current]\n",
    "\n",
    "        # skip series full of missing values\n",
    "        if data_current.isnull().all():\n",
    "            continue\n",
    "                \n",
    "        # deal with potential missing values in partially filled series\n",
    "        # if in scale 1-10, impossible to infer the values, replace all values of this serie by NaNs and skip\n",
    "        # if in shared scale 0-100\n",
    "        # assign 0 to missing values if the sum of the serie is already equal to 100\n",
    "        # otherwise impossible to infer the values, replace all values of this serie by NaNs and skip\n",
    "        if (data_current.isnull().any()) & (data_current.max() <= 10):\n",
    "            data4.loc[j,columns_current] = np.nan\n",
    "            continue\n",
    "        if (data_current.isnull().any()) & (data_current.max() > 10) & (data_current.sum() == 100):\n",
    "            data_current[data_current.isnull()] = 0\n",
    "        if (data_current.isnull().any()) & (data_current.max() > 10) & (data_current.sum() != 100):\n",
    "            data4.loc[j,columns_current] = np.nan\n",
    "            continue\n",
    "          \n",
    "        # decipher between max error on scale 1-10 and min error on shared scale 0-100\n",
    "        # consider serie as belonging to shared 0-100 scale if its sum is over 75\n",
    "        if (data_current.max() > 10) & (data_current.sum() <= 75):\n",
    "            data_current[data_current > 10] = 10\n",
    "\n",
    "        # if the sum of a serie is equal to 0, assign the minimum value of 1\n",
    "        if data_current.sum() <= 0:\n",
    "            data_current[:] = 1\n",
    "            \n",
    "        # apply corrections on min\n",
    "        if (data_current.max() <= 10) & (data_current.min() < 1):\n",
    "            data_current[data_current < 1] = 1\n",
    "        if (data_current.max() > 10) & (data_current.min() < 0):\n",
    "            data_current[data_current < 0] = 0\n",
    "        \n",
    "        # rescale scale 1-10 to shared scale 0-100\n",
    "        if data_current.sum() == 100:\n",
    "            data4.loc[j,columns_current] = data_current\n",
    "        else:\n",
    "            data4.loc[j,columns_current] = data_current * 100 / data_current.sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3 - preprocessing - correct values that were rescaled ### ----\n",
    "\n",
    "# many values were standardised, leading to non-integer values\n",
    "# - set all values to integers (this will result in errors in the serie total)\n",
    "# - apply following corrections:\n",
    "# if serie sum = 98, add 2 to the min value\n",
    "# if serie sum = 99, add 1 to the min value\n",
    "# if serie sum = 101, remove 1 to the max value\n",
    "# if serie sum = 102, remove 2 to the max value\n",
    "\n",
    "# copy data for safety\n",
    "data5 = data4.copy()\n",
    "\n",
    "# round values\n",
    "data5 = np.round(data5)\n",
    "\n",
    "# apply corrections on all series\n",
    "# loop through rows\n",
    "for i in range(0,data5.shape[0]):\n",
    "\n",
    "    # loop through quotation series\n",
    "    for j in range(5,series_all.shape[0]):\n",
    "\n",
    "        # get serie columns\n",
    "        serie_columns = series_all.loc[j, series_all.loc[j,:].notnull()]\n",
    "\n",
    "        # get data\n",
    "        serie_current = data5.loc[i,serie_columns]\n",
    "\n",
    "        # apply corrections\n",
    "        if serie_current.isnull().all():\n",
    "            continue\n",
    "        elif serie_current.isnull().any():\n",
    "            print(\"bug\")\n",
    "        elif serie_current.sum() == 100:\n",
    "            continue\n",
    "        elif serie_current.sum() == 98:\n",
    "            index_min = serie_current.index[np.argmin(serie_current)]\n",
    "            data5.loc[i,index_min] += 2\n",
    "        elif serie_current.sum() == 99:\n",
    "            index_min = serie_current.index[np.argmin(serie_current)]\n",
    "            data5.loc[i,index_min] += 1\n",
    "        elif serie_current.sum() == 101:\n",
    "            index_max = serie_current.index[np.argmin(serie_current)]\n",
    "            data5.loc[i,index_max] -= 1\n",
    "        elif serie_current.sum() == 102:\n",
    "            index_max = serie_current.index[np.argmin(serie_current)]\n",
    "            data5.loc[i,index_max] -= 2\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3 - preprocessing - indentify and treat outliers ### ----\n",
    "\n",
    "# done within each gender, not to indroduce a bias since features will be compared within genders\n",
    "# - for numeric features, drop the rows if value is below or over 2 * std\n",
    "# - for categorical features, set their value to \"Under-represented\"\n",
    "\n",
    "\n",
    "# copy data for safety\n",
    "data6 = data5.copy()\n",
    "\n",
    "# check outliers only in features related to demographics info\n",
    "columns_check_num = ['age_o', 'age', 'mn_sat', 'tuition', 'income']\n",
    "columns_check_cat = ['race_o', 'field', 'field_cd', 'undergra', 'race', 'from', 'zipcode', 'career', 'career_c']\n",
    "\n",
    "# set masks for gender\n",
    "mask_f = data6[\"gender\"] == \"Female\"\n",
    "mask_m = data6[\"gender\"] == \"Male\"\n",
    "\n",
    "# drop rows that contain numeric outliers\n",
    "for i in columns_check_num:\n",
    "\n",
    "    # get lower and upper bonds per gender\n",
    "    lower_bound_current_f = data6.loc[mask_f,i].mean() - 2 * data6.loc[mask_f,i].std()\n",
    "    upper_bound_current_f = data6.loc[mask_f,i].mean() + 2 * data6.loc[mask_f,i].std()\n",
    "    lower_bound_current_m = data6.loc[mask_m,i].mean() - 2 * data6.loc[mask_m,i].std()\n",
    "    upper_bound_current_m = data6.loc[mask_m,i].mean() + 2 * data6.loc[mask_m,i].std()\n",
    "    \n",
    "    # set masks for outliers and get id of the corresponding subjects\n",
    "    mask_outliers_f = (data6.loc[:,i] < lower_bound_current_f) | (data6.loc[:,i] > upper_bound_current_f)\n",
    "    mask_outliers_m = (data6.loc[:,i] < lower_bound_current_m) | (data6.loc[:,i] > upper_bound_current_m)\n",
    "    mask_outliers = mask_outliers_f | mask_outliers_m\n",
    "    id_outliers = data6.loc[mask_outliers,\"iid\"]\n",
    "\n",
    "    # drop rows that contain outliers\n",
    "    mask_drop = (data6.loc[:,\"iid\"].isin(id_outliers)) | (data6.loc[:,\"pid\"].isin(id_outliers))\n",
    "    index_drop = data6.loc[mask_drop,:].index\n",
    "    data6 = data6.drop(index_drop, axis = 0)\n",
    "    \n",
    "# assign new value to outliers in categorical features (categories representing less than 1% of the gender)\n",
    "for i in columns_check_cat:\n",
    "\n",
    "    # get category frequencies (without NaNs) and set mask for frequencies < 0.01\n",
    "    freq_current_f = data6.loc[mask_f,i].value_counts() / data6.loc[mask_f,i].value_counts().sum()\n",
    "    categories_current_f = freq_current_f.index[freq_current_f.values < 0.01]\n",
    "    mask_current_f = mask_f & (data6[i].isin(categories_current_f))\n",
    "    freq_current_m = data6.loc[mask_m,i].value_counts() / data6.loc[mask_m,i].value_counts().sum()\n",
    "    categories_current_m = freq_current_m.index[freq_current_m.values < 0.01]\n",
    "    mask_current_m = mask_m & (data6[i].isin(categories_current_m))\n",
    "\n",
    "    # set new value to outliers\n",
    "    data6.loc[mask_current_f,i] = \"Under-represented\"\n",
    "    data6.loc[mask_current_m,i] = \"Under-represented\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3 - preprocessing - reformat data to create a new dataset (one row per subject) ### ----\n",
    "\n",
    "# the dataset covers dates between 551 subjects within 21 waves\n",
    "# a lot of data is redundant (multiple speed dates per subject)\n",
    "\n",
    "# not all subjects had the same number of speed dates\n",
    "# the dataset is biased towards subjects that had more dates\n",
    "\n",
    "# this dataset will be used to plot the demographics of the cohort (and not of the survey)\n",
    "\n",
    "# - keep only features that are interesting for plotting some demographics info\n",
    "# - get data grouped by subject\n",
    "\n",
    "\n",
    "# keep interesting features\n",
    "columns_keep = [\"iid\", \"gender\", \"age\", \"race\", \"income\"]\n",
    "data7 = data6[columns_keep]\n",
    "\n",
    "# get subjects id and number of subjects\n",
    "subjects_id = data7[\"iid\"].unique()\n",
    "subjects_nb = len(subjects_id)\n",
    "\n",
    "# initialise new dataset (with columns to store like and match info)\n",
    "data_fig = pd.DataFrame(np.nan, index = range(0,subjects_nb), columns = data7.columns)\n",
    "\n",
    "# loop through subjects to collect individual data\n",
    "for i in range(0,subjects_nb):\n",
    "\n",
    "    # store data on subjects (take the first record of each subject)\n",
    "    data_current = data7.loc[data7[\"iid\"] == subjects_id[i],:]\n",
    "    data_fig.loc[i,columns_keep] = data_current.loc[data_current.index[0],columns_keep]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\n",
    "### 4 - Overview 1 - Quality of the speed dating experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4 - overview 1 - quality of the speed dating experiment - get data ### ----\n",
    "\n",
    "# the dataset contains recordings of information given at several time points of\n",
    "# the experiment by the subjects\n",
    "\n",
    "# 1 - assess quality of the surveys\n",
    "# some general info on subjects may be missing\n",
    "# some subjects may not have filled all the surveys\n",
    "\n",
    "# 2 - assess effectiveness of the dating process\n",
    "# number of likes\n",
    "# number of matches\n",
    "# number of dates after the experiment\n",
    "\n",
    "\n",
    "# 1 - assess quality of the surveys\n",
    "# get percentage of fully and partially filled surveys\n",
    "\n",
    "# store info available on each subject by survey\n",
    "general = [\"iid\",\"id\",\"gender\",\"idg\",\"condtn\",\"wave\",\"round\",\"positin1\"]\n",
    "signup1 = [\"age\",\"field\",\"field_cd\",\"undergra\",\"mn_sat\",\"tuition\",\"race\",\"imprace\",\"imprelig\",\"from\",\n",
    "       \"zipcode\",\"income\",\"goal\",\"date\",\"go_out\",\"career\",\"career_c\",\"sports\",\"tvsports\",\"exercise\",\n",
    "       \"dining\",\"museums\",\"art\",\"hiking\",\"gaming\",\"clubbing\",\"reading\",\"tv\",\"theater\",\"movies\",\n",
    "       \"concerts\",\"music\",\"shopping\",\"yoga\",\"exphappy\",\"expnum\",\"attr1_1\",\"sinc1_1\",\"intel1_1\",\"fun1_1\",\n",
    "       \"amb1_1\",\"shar1_1\",\"attr4_1\",\"sinc4_1\",\"intel4_1\",\"fun4_1\",\"amb4_1\",\"shar4_1\",\"attr2_1\",\"sinc2_1\",\n",
    "       \"intel2_1\",\"fun2_1\",\"amb2_1\",\"shar2_1\",\"attr3_1\",\"sinc3_1\",\"fun3_1\",\"intel3_1\",\"amb3_1\",\"attr5_1\",\n",
    "       \"sinc5_1\",\"intel5_1\",\"fun5_1\",\"amb5_1\"]\n",
    "scorecard = [\"dec\",\"attr\",\"sinc\",\"intel\",\"fun\",\"amb\",\"shar\",\"like\",\"prob\",\"met\",\"match_es\"]\n",
    "signup2 = [\"attr1_s\",\"sinc1_s\",\"intel1_s\",\"fun1_s\",\"amb1_s\",\"shar1_s\",\"attr3_s\",\"sinc3_s\",\"intel3_s\",\n",
    "       \"fun3_s\",\"amb3_s\"]\n",
    "       \n",
    "followup1 = ['satis_2', 'length', 'numdat_2', 'attr7_2', 'sinc7_2', 'intel7_2', \n",
    "       'fun7_2', 'amb7_2', 'shar7_2', 'attr1_2', 'sinc1_2', 'intel1_2', 'fun1_2',\n",
    "       'amb1_2', 'shar1_2', 'attr4_2', 'sinc4_2', 'intel4_2', 'fun4_2', 'amb4_2', \n",
    "       'shar4_2', 'attr2_2', 'sinc2_2', 'intel2_2', 'fun2_2', 'amb2_2', 'shar2_2',\n",
    "       'attr3_2', 'sinc3_2', 'intel3_2', 'fun3_2', 'amb3_2', 'attr5_2', 'sinc5_2', \n",
    "       'intel5_2', 'fun5_2', 'amb5_2']\n",
    "       \n",
    "followup2 = ['you_call', 'them_cal', 'date_3', 'numdat_3', 'num_in_3', 'attr1_3', \n",
    "       'sinc1_3', 'intel1_3', 'fun1_3', 'amb1_3', 'shar1_3', 'attr7_3', 'sinc7_3', \n",
    "       'intel7_3', 'fun7_3', 'amb7_3', 'shar7_3', 'attr4_3', 'sinc4_3', 'intel4_3', \n",
    "       'fun4_3', 'amb4_3', 'shar4_3', 'attr2_3', 'sinc2_3', 'intel2_3', 'fun2_3', \n",
    "       'amb2_3', 'shar2_3', 'attr3_3', 'sinc3_3', 'intel3_3', 'fun3_3', 'amb3_3', \n",
    "       'attr5_3', 'sinc5_3', 'intel5_3', 'fun5_3', 'amb5_3']\n",
    "\n",
    "# store all infos\n",
    "infos_all = general + signup1 + scorecard + signup2 + followup1 + followup2\n",
    "infos_all_df = pd.DataFrame(index = [\"general<br>info\",\"signup1\",\"scorecard\",\"signup2\",\"followup1\",\"followup2\"],\n",
    "                                columns = np.arange(0,len(signup1)))\n",
    "infos_all_df.iloc[0,0:len(general)] = general\n",
    "infos_all_df.iloc[1,0:len(signup1)] = signup1\n",
    "infos_all_df.iloc[2,0:len(scorecard)] = scorecard\n",
    "infos_all_df.iloc[3,0:len(signup2)] = signup2\n",
    "infos_all_df.iloc[4,0:len(followup1)] = followup1\n",
    "infos_all_df.iloc[5,0:len(followup2)] = followup2\n",
    "\n",
    "# initialise variables to store summary results (\"not filled\", \"partially filled\", \"fully filled\")\n",
    "data_fig1a = pd.DataFrame(np.zeros((6,4)), columns = [\"info\",\"count not\",\"count partial\",\"count full\"])\n",
    "data_fig1a[\"info\"] = infos_all_df.index\n",
    "\n",
    "# loop through dates\n",
    "for i in range(0,data.shape[0]): \n",
    "       \n",
    "       # loop through type of info\n",
    "       for j in range(0,infos_all_df.shape[0]):\n",
    "\n",
    "              # get type of info to be tested\n",
    "              infos_current = infos_all_df.iloc[j,:]\n",
    "              infos_current = infos_current[infos_current.notnull()]\n",
    "\n",
    "              # get data for current subject and current info\n",
    "              data_current = data.loc[i,infos_current]\n",
    "\n",
    "              # test for NaN content\n",
    "              if data_current.isnull().all():\n",
    "                     data_fig1a.loc[j,\"count not\"] += 1\n",
    "              elif data_current.notnull().all():\n",
    "                     data_fig1a.loc[j,\"count full\"] += 1\n",
    "              else:\n",
    "                     data_fig1a.loc[j,\"count partial\"] += 1\n",
    "\n",
    "# update info summary with percentages\n",
    "data_fig1a[\"percent not\"] = data_fig1a[\"count not\"] * 100 / data.shape[0]\n",
    "data_fig1a[\"percent partial\"] = data_fig1a[\"count partial\"] * 100 / data.shape[0]\n",
    "data_fig1a[\"percent full\"] = data_fig1a[\"count full\"] * 100 / data.shape[0]\n",
    "\n",
    "\n",
    "# 2 - assess effectiveness of the dating process\n",
    "# get percentage of successfull dating steps\n",
    "\n",
    "# initialise variable to store data to plot\n",
    "data_fig1b= pd.DataFrame(index = [\"percent\"], columns = [\"likes\",\"matches\",\"dates\"])\n",
    "\n",
    "# get percent over number of total number of records\n",
    "data_fig1b.loc[\"percent\",\"likes\"] = (data[\"dec\"] == 1).sum() * 100 / data.shape[0]\n",
    "data_fig1b.loc[\"percent\",\"matches\"] = (data[\"match\"] == 1).sum() * 100 / data.shape[0]\n",
    "dates_total = 0\n",
    "for i in data[\"iid\"].unique():\n",
    "       if data.loc[data[\"iid\"] == i,\"numdat_3\"].isnull().all():\n",
    "              continue\n",
    "       else:\n",
    "              dates_total += data.loc[data[\"iid\"] == i,\"numdat_3\"].unique()\n",
    "data_fig1b.loc[\"percent\",\"dates\"] = dates_total[0] * 100 / data.shape[0]\n",
    "\n",
    "\n",
    "# 3 - assess importance of speed date order to get a like and a date\n",
    "\n",
    "# get data\n",
    "data_fig1c = data.loc[:,[\"order\", \"dec\", \"num_in_3\"]]\n",
    "data_fig1c.loc[data_fig1c[\"dec\"] == \"Yes\"] = 1\n",
    "data_fig1c.loc[data_fig1c[\"dec\"] == \"No\"] = 0\n",
    "\n",
    "# get order, likes mean and dates mean per date order\n",
    "order = data_fig1c[\"order\"].sort_values().unique()\n",
    "likes_mean = data_fig1c.groupby([\"order\"])[\"dec\"].mean()\n",
    "dates_mean = data_fig1c.groupby([\"order\"])[\"num_in_3\"].mean()\n",
    "\n",
    "# get fits\n",
    "likes_popt, pcov = curve_fit(lambda x, a, b: a * x + b, order, likes_mean)\n",
    "likes_fit = likes_popt[0] * order + likes_popt[1]\n",
    "dates_popt, pcov = curve_fit(lambda x, a, b, c: a * np.exp(b * x) + c, order, dates_mean, p0 = (0.85, 1.4, 0.8))\n",
    "dates_fit = dates_popt[0] * np.exp(dates_popt[1] * order) + dates_popt[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4 - overview 1 - quality of the speed dating experiment - plot ### ----\n",
    "\n",
    "# set figure to make subplots\n",
    "fig1 = make_subplots(\n",
    "    rows = 2,\n",
    "    cols = 6,\n",
    "    specs = [[{\"colspan\": 4}, None, None, None, {\"colspan\": 2}, None], \n",
    "    [{\"colspan\": 3}, None, None, {\"colspan\": 3}, None, None]],\n",
    "    subplot_titles = (\"A. Surveys\",\n",
    "                        \"B. Dating steps\",\n",
    "                        \"C. Speed date order and likes given\",\n",
    "                        \"D. Speed date order and actual dates\"),\n",
    "    row_heights = [0.5, 0.35],\n",
    "    vertical_spacing = 0.15,\n",
    "    horizontal_spacing = 0.18)\n",
    "\n",
    "# plot percentage of fully and partially filled surveys\n",
    "fig1.add_trace(go.Bar(\n",
    "    name='Fully filled',\n",
    "    x = data_fig1a[\"info\"],\n",
    "    y = data_fig1a[\"percent full\"],\n",
    "    marker_color = px.colors.qualitative.Vivid[2],\n",
    "    showlegend = True),\n",
    "    row = 1, col = 1)\n",
    "\n",
    "fig1.add_trace(go.Bar(\n",
    "    name='Partially filled',\n",
    "    x = data_fig1a[\"info\"],\n",
    "    y = data_fig1a[\"percent partial\"],\n",
    "    marker_color = px.colors.qualitative.Vivid[3],\n",
    "    showlegend = True),\n",
    "    row = 1, col = 1)\n",
    "\n",
    "# plot percentage of successfull dating steps\n",
    "fig1.add_trace(go.Bar(\n",
    "    x = data_fig1b.columns,\n",
    "    y = data_fig1b.loc[\"percent\",:],\n",
    "    marker_color = px.colors.qualitative.Vivid[4:],\n",
    "    text = data_fig1b.loc[\"percent\",:],\n",
    "    texttemplate= \"%{text:.1f}\",\n",
    "    textfont = dict(color = [\"rgb(232,232,232)\", \"rgb(232,232,232)\"]),\n",
    "    showlegend = False),\n",
    "    row = 1, col = 5)\n",
    "\n",
    "# plot percentage of likes\n",
    "fig1.add_trace(go.Scatter(\n",
    "    x = order, \n",
    "    y = likes_mean,\n",
    "    marker_color = px.colors.qualitative.Vivid[7],\n",
    "    mode = \"markers\",\n",
    "    showlegend = False),\n",
    "    row = 2, col = 1)\n",
    "fig1.add_trace(go.Scatter(\n",
    "    x = order, \n",
    "    y = likes_fit, \n",
    "    marker_color = px.colors.qualitative.Vivid[9],\n",
    "    mode = \"lines\",\n",
    "    showlegend = False),\n",
    "    row = 2, col = 1)\n",
    "\n",
    "# plot number of dates\n",
    "fig1.add_trace(go.Scatter(\n",
    "    x = order, \n",
    "    y = dates_mean, \n",
    "    marker_color = px.colors.qualitative.Vivid[8],\n",
    "    mode = \"markers\",\n",
    "    showlegend = False),\n",
    "    row = 2, col = 4)\n",
    "fig1.add_trace(go.Scatter(\n",
    "    x = order, \n",
    "    y = dates_fit, \n",
    "    marker_color = px.colors.qualitative.Vivid[9],\n",
    "    mode = \"lines\",\n",
    "    showlegend = False),\n",
    "    row = 2, col = 4)\n",
    "\n",
    "# update layout\n",
    "fig1.update_annotations(font_size = 15)\n",
    "fig1.update_xaxes(title_font = dict(size = 13), tickfont = dict(size = 11))\n",
    "fig1.update_yaxes(title_font = dict(size = 13), tickfont = dict(size = 11))\n",
    "fig1.update_layout(\n",
    "    margin = dict(l = 90),\n",
    "    title_text = \"Figure 1. Quality of the speed dating experiment\",\n",
    "    title_x = 0.5,\n",
    "    title_y = 0.95,\n",
    "    title_font_size = 18,\n",
    "    barmode = \"stack\",\n",
    "    xaxis3 = dict(title = \"Speed date order\", zeroline = False, showgrid = False),\n",
    "    xaxis4 = dict(title = \"Speed date order\", zeroline = False, showgrid = False),\n",
    "    yaxis = dict(title = \"Percent of total records\", range = [0,110], tickvals = [0, 20, 40, 60, 80, 100]),\n",
    "    yaxis2 = dict(title = \"Percent of total records\", range = [0,55], tickvals = [0, 10, 20, 30, 40, 50]),\n",
    "    yaxis3 = dict(title = \"Ratio of likes (over speed date number)\", range = [0.28, 0.55], \n",
    "        tickvals = [0.30, 0.35, 0.40, 0.45, 0.50]),\n",
    "    yaxis4 = dict(title = \"Number of actual dates (mean)\", range = [0.76, 1.3], \n",
    "        tickvals = [0.8, 0.9, 1.0, 1.1, 1.2]),\n",
    "    legend = dict(\n",
    "        orientation = \"h\",\n",
    "        yanchor = \"top\",\n",
    "        y = 0.47,\n",
    "        xanchor = \"left\",\n",
    "        x = 0.12),\n",
    "    plot_bgcolor = \"rgba(0,0,0,0)\",\n",
    "    paper_bgcolor = \"rgb(232,232,232)\",\n",
    "    width = 800,\n",
    "    height = 800)\n",
    "\n",
    "fig1.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\n",
    "### 5 - Overview 2 - Demographics of the cohort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 5 - overview 2 - demographics of the cohort - plot ### ----\n",
    "\n",
    "# format data for races\n",
    "counts = data_fig[\"race\"].value_counts()\n",
    "races = counts.index\n",
    "\n",
    "# set figure to make subplots for each region\n",
    "fig2 = make_subplots(rows = 2, cols = 2,\n",
    "                        specs = [[{\"type\": \"pie\"}, {\"type\": \"box\"}], [{\"type\": \"pie\"}, {\"type\": \"box\"}]],\n",
    "                        subplot_titles = (\"A. Gender representation\",\n",
    "                                                \"B. Age profile per gender\",\n",
    "                                                \"C. Ethnicity representation\",\n",
    "                                                \"D. Income profile per ethnicity\"),\n",
    "                        column_widths = [0.3, 0.4],\n",
    "                        horizontal_spacing = 0.3,\n",
    "                        vertical_spacing = 0.15)\n",
    "\n",
    "# plot gender representation\n",
    "fig2.add_trace(go.Pie(\n",
    "        labels = data_fig[\"gender\"].value_counts(dropna = False).index,\n",
    "        values = data_fig[\"gender\"].value_counts(dropna = False).values,\n",
    "        textfont = dict(size = 12),\n",
    "        textinfo = \"label + percent\",\n",
    "        textposition = \"outside\",\n",
    "        hole = 0.3,\n",
    "        showlegend = False,\n",
    "        marker = dict(colors = px.colors.qualitative.Vivid)),\n",
    "\n",
    "        row = 1, col = 1)\n",
    "\n",
    "# plot age profile per gender\n",
    "fig2.add_trace(go.Box(\n",
    "        y = data_fig.loc[data_fig[\"gender\"] == \"Female\",\"age\"],\n",
    "        name = \"Female\",\n",
    "        marker_color = px.colors.qualitative.Vivid[1]),\n",
    "        row = 1, col = 2)\n",
    "fig2.add_trace(go.Box(\n",
    "        y = data_fig.loc[data_fig[\"gender\"] == \"Male\",\"age\"],\n",
    "        name = \"Male\",\n",
    "        marker_color = px.colors.qualitative.Vivid[0]),\n",
    "        row = 1, col = 2)\n",
    "\n",
    "# plot ethnicity representation\n",
    "fig2.add_trace(go.Pie(\n",
    "        labels = data_fig[\"race\"].value_counts(dropna = False).index,\n",
    "        values = data_fig[\"race\"].value_counts(dropna = False).values,\n",
    "        textfont = dict(size = 12),\n",
    "        textinfo = \"label + percent\",\n",
    "        textposition = \"outside\",\n",
    "        rotation = 0,\n",
    "        hole = 0.3,\n",
    "        showlegend = False,\n",
    "        marker = dict(colors = px.colors.qualitative.Vivid[2:])),\n",
    "\n",
    "        row = 2, col = 1)\n",
    "\n",
    "# plot income profile per ethnicity\n",
    "[fig2.add_trace(go.Box(\n",
    "        y = data_fig.loc[data_fig[\"race\"] == races[i],\"income\"], \n",
    "        name = races[i], \n",
    "        marker_color = px.colors.qualitative.Vivid[i+2]),\n",
    "        row = 2, col = 2) for i in range (0,len(races))]\n",
    "\n",
    "# update layout\n",
    "fig2.update_annotations(font_size = 15)\n",
    "fig2.update_xaxes(title_font = dict(size = 13), tickfont = dict(size = 12))\n",
    "fig2.update_yaxes(title_font = dict(size = 13), tickfont = dict(size = 12))\n",
    "fig2.update_layout(\n",
    "        margin = dict(l = 90),\n",
    "        title_text = \"Figure 2. Demographics of the cohort\",\n",
    "        title_x = 0.5,\n",
    "        title_y = 0.95,\n",
    "        title_font_size = 18,\n",
    "        yaxis = dict(title = \"Age\", range = [13,40], tickvals = [15, 20, 25, 30, 35]),\n",
    "        yaxis2 = dict(title = \"Income\", range = [-10000,120000], \n",
    "                tickvals = [0, 20000, 40000, 60000, 80000, 100000]), \n",
    "        showlegend = False,\n",
    "        plot_bgcolor = \"rgba(0,0,0,0)\",\n",
    "        paper_bgcolor = \"rgb(232,232,232)\",\n",
    "        width = 800,\n",
    "        height = 800)\n",
    "\n",
    "fig2.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\n",
    "### 6 - Analysis 1 - Importance and impact of partner's attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 6 - analysis 1 - importance and impact of partner's attributes - get data ### ----\n",
    "\n",
    "# compare the importance given to a serie of attributes before the speed dates\n",
    "\n",
    "# assess the impact of the most desired attributes (for females and males)\n",
    "# impact is assessed by comparing the means (t-test) of the grades given to the partner after \n",
    "# the speed date for the given attribute, depending on whether they obtained a like or not\n",
    "\n",
    "\n",
    "# keep only columns that are relevant for the analysis\n",
    "columns_useful = ['gender', 'dec', \"intel\", \"attr\", 'attr1_1', 'sinc1_1', 'intel1_1', 'fun1_1', 'amb1_1', 'shar1_1']\n",
    "data_fig3 = data6[columns_useful]\n",
    "\n",
    "# rename columns for nice plotting\n",
    "columns_names = [\"Gender\", \"Like\", \"Intel_Rating\", \"Attr_Rating\", \"Attractiveness\", \"Sincerity\", \"Intelligence\",\n",
    "    \"Fun\", \"Ambition\", \"Shared<br>interests\"]\n",
    "data_fig3.columns = columns_names\n",
    "\n",
    "# set masks for genders\n",
    "mask_f = data_fig3[\"Gender\"] == \"Female\" \n",
    "mask_m = data_fig3[\"Gender\"] == \"Male\"\n",
    "\n",
    "# sort attributes by ascending median for plotting\n",
    "attributes_f = data_fig3.loc[mask_f,columns_names[4:]].median().sort_values()\n",
    "attributes_m = data_fig3.loc[mask_m,columns_names[4:]].median().sort_values()\n",
    "\n",
    "# set masks for genders and likes given to partner\n",
    "mask_f_yes = (data_fig3[\"Gender\"] == \"Female\") & (data_fig3[\"Like\"] == \"Yes\")\n",
    "mask_f_no = (data_fig3[\"Gender\"] == \"Female\") & (data_fig3[\"Like\"] == \"No\")\n",
    "mask_m_yes = (data_fig3[\"Gender\"] == \"Male\") & (data_fig3[\"Like\"] == \"Yes\")\n",
    "mask_m_no = (data_fig3[\"Gender\"] == \"Male\") & (data_fig3[\"Like\"] == \"No\")\n",
    "\n",
    "# assess the impact of male intelligence for getting a like\n",
    "_, pvalue_intel = stats.ttest_ind(\n",
    "    data_fig3.loc[mask_f_yes,\"Intel_Rating\"], data_fig3.loc[mask_f_no,\"Intel_Rating\"],\n",
    "    equal_var = False, nan_policy = \"omit\", alternative = \"greater\")\n",
    "\n",
    "# assess the impact of female attractiveness for getting a like\n",
    "_, pvalue_attr = stats.ttest_ind(\n",
    "    data_fig3.loc[mask_m_yes,\"Attr_Rating\"], data_fig3.loc[mask_m_no,\"Attr_Rating\"],\n",
    "    equal_var = False, nan_policy = \"omit\", alternative = \"greater\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 6 - analysis 1 - importance and impact of partner's attributes - plot ### ----\n",
    "\n",
    "# set figure to make subplots\n",
    "fig3 = make_subplots(\n",
    "    rows = 2,\n",
    "    cols = 2,\n",
    "    subplot_titles = (\n",
    "        \"A. Importance of attributes<br>as rated by females\",\n",
    "        \"B. Impact of Intelligence on<br>female decision (pvalue = {:.4f})\".format(pvalue_intel),\n",
    "        \"C. Importance of attributes<br>as rated by males\",\n",
    "        \"D. Impact of Attractiveness on<br>male decision (pvalue = {:.4f})\".format(pvalue_attr)),\n",
    "    row_heights = [0.40, 0.40],\n",
    "    column_widths = [0.6, 0.2],\n",
    "    horizontal_spacing = 0.15,\n",
    "    vertical_spacing = 0.15)\n",
    "\n",
    "# plot importance for females\n",
    "[fig3.add_trace(go.Box(\n",
    "        y = data_fig3.loc[mask_f,i], \n",
    "        name = i, \n",
    "        marker_color = px.colors.qualitative.Vivid[1]),\n",
    "        row = 1, col = 1) for i in attributes_f.index]\n",
    "\n",
    "# plot importance for males\n",
    "[fig3.add_trace(go.Box(\n",
    "        y = data_fig3.loc[mask_m,i], \n",
    "        name = i, \n",
    "        marker_color = px.colors.qualitative.Vivid[0]),\n",
    "        row = 2, col = 1) for i in attributes_m.index]\n",
    "\n",
    "# plot impact of intelligence\n",
    "fig3.add_trace(go.Box(\n",
    "        y = data_fig3.loc[mask_f_no,\"Intel_Rating\"],\n",
    "        name = \"No Like\",\n",
    "        marker_color = px.colors.qualitative.Vivid[1]),\n",
    "        row = 1, col = 2)\n",
    "fig3.add_trace(go.Box(\n",
    "        y = data_fig3.loc[mask_f_yes,\"Intel_Rating\"],\n",
    "        name = \"Like\",\n",
    "        marker_color = px.colors.qualitative.Vivid[1]),\n",
    "        row = 1, col = 2)\n",
    "\n",
    "# plot impact of attractiveness\n",
    "fig3.add_trace(go.Box(\n",
    "        y = data_fig3.loc[mask_m_no,\"Attr_Rating\"],\n",
    "        name = \"No Like\",\n",
    "        marker_color = px.colors.qualitative.Vivid[0]),\n",
    "        row = 2, col = 2)\n",
    "fig3.add_trace(go.Box(\n",
    "        y = data_fig3.loc[mask_m_yes,\"Attr_Rating\"],\n",
    "        name = \"Like\",\n",
    "        marker_color = px.colors.qualitative.Vivid[0]),\n",
    "        row = 2, col = 2)\n",
    "\n",
    "# update layout\n",
    "fig3.update_annotations(font_size = 15)\n",
    "fig3.update_xaxes(title_font = dict(size = 13), tickfont = dict(size = 10))\n",
    "fig3.update_yaxes(title_font = dict(size = 13), tickfont = dict(size = 10))\n",
    "fig3.update_layout(\n",
    "        margin = dict(l = 90, t = 130),\n",
    "        title_text = \"Figure 3. Importance and impact of partner's attributes\",\n",
    "        title_x = 0.5,\n",
    "        title_y = 0.95,\n",
    "        title_font_size = 18, \n",
    "        yaxis = dict(title = \"Rating (scale 0-100)\", range = [-10, 120], tickvals = [0, 20, 40, 60, 80, 100]),\n",
    "        yaxis2 = dict(title = \"Rating (scale 1-10)\", range = [-1, 12], tickvals = [0, 2, 4, 6, 8, 10]),\n",
    "        yaxis3 = dict(title = \"Rating (scale 0-100)\", range = [-10, 120], tickvals = [0, 20, 40, 60, 80, 100]),\n",
    "        yaxis4 = dict(title = \"Rating (scale 1-10)\", range = [-1, 12], tickvals = [0, 2, 4, 6, 8, 10]),\n",
    "        showlegend = False,\n",
    "        plot_bgcolor = \"rgba(0,0,0,0)\",\n",
    "        paper_bgcolor = \"rgb(232,232,232)\",\n",
    "        width = 800,\n",
    "        height = 800)\n",
    "\n",
    "fig3.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\n",
    "### 7 - Analysis 2 - Impact of shared interests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 7 - analysis 2 - impact of shared interests - get data ### ----\n",
    "\n",
    "# the feature int_corr indicates whether partners share interests or not but is not quantitative\n",
    "# to get a more informative feature, calculate the Euclidean distance between partners (relative to activities)\n",
    "\n",
    "# assess the impact of having shared interests on giving a like to the partner\n",
    "\n",
    "# assess the impact of having shared interests on getting a match between partners\n",
    "\n",
    "\n",
    "# keep only columns that are relevant for the analysis\n",
    "columns_useful = ['iid', 'pid', 'gender', 'dec', 'match', 'sports', 'tvsports', 'exercise', 'dining', \n",
    "    'museums', 'art', 'hiking', 'gaming', 'clubbing', 'reading', 'tv', 'theater', 'movies', 'concerts', 'music', \n",
    "    'shopping', 'yoga']\n",
    "data_fig4 = data6[columns_useful].reset_index(drop = True)\n",
    "\n",
    "# rename columns for nice plotting\n",
    "columns_names = [\"iid\", \"pid\", \"Gender\", \"Like\", \"Match\", \"Sports\", \"TV-Sports\", \"Exercise\", \"Dining\", \n",
    "    \"Museums\", \"Art\", \"Hiking\", \"Gaming\", \"Clubbing\", \"Reading\", \"TV\", \"Theater\", \"Movies\", \"Concerts\", \"Music\", \n",
    "    \"Shopping\", \"Yoga\"]\n",
    "data_fig4.columns = columns_names\n",
    "\n",
    "# initialize columns to store distance\n",
    "data_fig4[\"Distance\"] = np.nan\n",
    "\n",
    "# loop through dates to get Euclidean distances between partners\n",
    "for i in range(0,data_fig4.shape[0]):\n",
    "\n",
    "    # get subject data\n",
    "    subject_activities = data_fig4.loc[i,columns_names[5:]]\n",
    "\n",
    "    # get partner data\n",
    "    partner_id = data_fig4.loc[i,\"pid\"]\n",
    "    partner_activities_all = data_fig4.loc[data_fig4[\"iid\"] == partner_id,columns_names[5:]].reset_index(drop = True)\n",
    "    partner_activities = partner_activities_all.loc[0,:]\n",
    "\n",
    "    # get euclidean distance\n",
    "    data_fig4.loc[i,\"Distance\"] = spatial.distance.euclidean(subject_activities, partner_activities)\n",
    "\n",
    "# set masks for genders and likes given to partner\n",
    "mask_f_yes = (data_fig4[\"Gender\"] == \"Female\") & (data_fig4[\"Like\"] == \"Yes\")\n",
    "mask_f_no = (data_fig4[\"Gender\"] == \"Female\") & (data_fig4[\"Like\"] == \"No\")\n",
    "mask_m_yes = (data_fig4[\"Gender\"] == \"Male\") & (data_fig4[\"Like\"] == \"Yes\")\n",
    "mask_m_no = (data_fig4[\"Gender\"] == \"Male\") & (data_fig4[\"Like\"] == \"No\")\n",
    "\n",
    "# assess the impact of shared interests for females to give a like\n",
    "_, pvalue_f = stats.ttest_ind(\n",
    "    data_fig4.loc[mask_f_yes,\"Distance\"], data_fig4.loc[mask_f_no,\"Distance\"],\n",
    "    equal_var = False, nan_policy = \"omit\", alternative = \"two-sided\")\n",
    "\n",
    "# assess the impact of shared interests for males to give a like\n",
    "_, pvalue_m = stats.ttest_ind(\n",
    "    data_fig4.loc[mask_m_yes,\"Distance\"], data_fig4.loc[mask_m_no,\"Distance\"],\n",
    "    equal_var = False, nan_policy = \"omit\", alternative = \"two-sided\")\n",
    "\n",
    "# copy data for safety\n",
    "data_fig4bis = data_fig4.loc[:,[\"iid\", \"pid\", \"Gender\", \"Match\", \"Distance\"]]\n",
    "\n",
    "# initialise variable to store index of data to drop\n",
    "index_drop = []\n",
    "\n",
    "# identify reciprocal speed dates\n",
    "for i in range(0,data_fig4bis.shape[0]):\n",
    "\n",
    "    # get subject and partner ids\n",
    "    # do it only for females, to not drop every date\n",
    "    if data_fig4bis.loc[i,\"Gender\"] == \"Female\":\n",
    "        subject_current = data_fig4bis.loc[i,\"iid\"]\n",
    "        partner_current = data_fig4bis.loc[i,\"pid\"]\n",
    "\n",
    "        # search for reciprocal speed date\n",
    "        mask_drop = (data_fig4bis[\"iid\"] == partner_current) & (data_fig4bis[\"pid\"] == subject_current)\n",
    "        index_drop.append(data_fig4bis.loc[mask_drop,:].index[0])\n",
    "\n",
    "# drop reciprocal speed dates\n",
    "data_fig4bis = data_fig4bis.drop(index_drop, axis = 0)\n",
    "\n",
    "# set masks for genders and likes given to partner\n",
    "mask_yesmatch = (data_fig4bis[\"Match\"] == \"Yes\")\n",
    "mask_nomatch = (data_fig4bis[\"Match\"] == \"No\")\n",
    "\n",
    "# assess the impact of shared interests for getting a match\n",
    "_, pvalue_match = stats.ttest_ind(\n",
    "    data_fig4bis.loc[mask_yesmatch,\"Distance\"], data_fig4bis.loc[mask_nomatch,\"Distance\"],\n",
    "    equal_var = False, nan_policy = \"omit\", alternative = \"two-sided\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 7 - analysis 1 - impact of shared interests - plot ### ----\n",
    "\n",
    "# set figure to make subplots\n",
    "fig4 = make_subplots(\n",
    "    rows = 1,\n",
    "    cols = 3,\n",
    "    subplot_titles = (\n",
    "        \"A. Impact  on female decision<br>(pvalue = {:.4f})\".format(pvalue_f),\n",
    "        \"B. Impact on male decision<br>(pvalue = {:.4f})\".format(pvalue_m),\n",
    "        \"C. Impact on matches<br>(pvalue = {:.4f})\".format(pvalue_match)),\n",
    "    column_widths = [0.25, 0.25, 0.25],\n",
    "    horizontal_spacing = 0.15)\n",
    "\n",
    "# plot impact of shared interests on female decision\n",
    "fig4.add_trace(go.Box(\n",
    "        y = data_fig4.loc[mask_f_no,\"Distance\"],\n",
    "        name = \"No Like\",\n",
    "        marker_color = px.colors.qualitative.Vivid[1]),\n",
    "        row = 1, col = 1)\n",
    "fig4.add_trace(go.Box(\n",
    "        y = data_fig4.loc[mask_f_yes,\"Distance\"],\n",
    "        name = \"Like\",\n",
    "        marker_color = px.colors.qualitative.Vivid[1]),\n",
    "        row = 1, col = 1)\n",
    "\n",
    "# plot impact of shared interests on male decision\n",
    "fig4.add_trace(go.Box(\n",
    "        y = data_fig4.loc[mask_m_no,\"Distance\"],\n",
    "        name = \"No Like\",\n",
    "        marker_color = px.colors.qualitative.Vivid[0]),\n",
    "        row = 1, col = 2)\n",
    "fig4.add_trace(go.Box(\n",
    "        y = data_fig4.loc[mask_m_yes,\"Distance\"],\n",
    "        name = \"Like\",\n",
    "        marker_color = px.colors.qualitative.Vivid[0]),\n",
    "        row = 1, col = 2)\n",
    "\n",
    "# plot impact of shared interests on matches\n",
    "fig4.add_trace(go.Box(\n",
    "        y = data_fig4bis.loc[mask_nomatch,\"Distance\"],\n",
    "        name = \"No Match\",\n",
    "        marker_color = px.colors.qualitative.Vivid[5]),\n",
    "        row = 1, col = 3)\n",
    "fig4.add_trace(go.Box(\n",
    "        y = data_fig4bis.loc[mask_yesmatch,\"Distance\"],\n",
    "        name = \"Match\",\n",
    "        marker_color = px.colors.qualitative.Vivid[5]),\n",
    "        row = 1, col = 3)\n",
    "\n",
    "# update layout\n",
    "fig4.update_annotations(font_size = 15)\n",
    "fig4.update_xaxes(title_font = dict(size = 13), tickfont = dict(size = 12))\n",
    "fig4.update_yaxes(title_font = dict(size = 13), tickfont = dict(size = 12))\n",
    "fig4.update_layout(\n",
    "        margin = dict(l = 90),\n",
    "        title_text = \"Figure 4. Impact of shared interests\",\n",
    "        title_x = 0.5,\n",
    "        title_y = 0.95,\n",
    "        title_font_size = 18,\n",
    "        yaxis = dict(title = \"Euclidean distance\", range = [-1, 35], tickvals = [0, 10, 20, 30]),\n",
    "        yaxis2 = dict(title = \"Euclidean distance\", range = [-1, 35], tickvals = [0, 10, 20, 30]),\n",
    "        yaxis3 = dict(title = \"Euclidean distance\", range = [-1, 35], tickvals = [0, 10, 20, 30]),\n",
    "        showlegend = False,\n",
    "        plot_bgcolor = \"rgba(0,0,0,0)\",\n",
    "        paper_bgcolor = \"rgb(232,232,232)\",\n",
    "        width = 800,\n",
    "        height = 400)\n",
    "\n",
    "fig4.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\n",
    "### 8 - Analysis 3 - Importance and impact of shared ethnicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 8 - analysis 3 - importance and impact of shared ethnicity - get data ### ----\n",
    "\n",
    "# the feature int_corr indicates whether partners share interests or not but is not quantitative\n",
    "# to get a more informative feature, calculate the Euclidean distance between partners (relative to activities)\n",
    "\n",
    "# assess the impact of having shared interests on giving a like to the partner\n",
    "\n",
    "\n",
    "# keep only columns that are relevant for the analysis\n",
    "columns_useful = ['iid', 'pid', 'gender', 'dec', 'match', 'race', 'samerace', 'imprace']\n",
    "data_fig5 = data6[columns_useful]\n",
    "\n",
    "# get importance of ethnicity per ethnicity\n",
    "race_imp = data_fig5.groupby([\"race\"])[\"imprace\"].median().sort_values()\n",
    "\n",
    "\n",
    "# get impact of having the same goal on decision to give a like and on getting a match\n",
    "\n",
    "# initialise variables to store results on impact\n",
    "data_fig5bc = pd.DataFrame(index = race_imp.index, columns = [\"percent_like\", \"percent_match\"])\n",
    "\n",
    "# set masks\n",
    "mask_f = data_fig5[\"gender\"] == \"Female\"\n",
    "mask_yeslike = data_fig5[\"dec\"] == \"Yes\"\n",
    "mask_yesmatch = data_fig5[\"match\"] == \"Yes\"\n",
    "mask_yessame = data_fig5[\"samerace\"] == \"Yes\"\n",
    "\n",
    "# fill dataframe with percentages\n",
    "for i in race_imp.index:\n",
    "\n",
    "    # for likes\n",
    "    data_fig5bc.loc[i,\"percent_like\"] = data_fig5.loc[(data_fig5[\"race\"] == i) & mask_yeslike & \\\n",
    "        mask_yessame,\"iid\"].count() / data_fig5.loc[(data_fig5[\"race\"] == i) & mask_yeslike,\"iid\"].count() * 100\n",
    "    \n",
    "    # for matches (only on females to drop reciprocal dates)\n",
    "    data_fig5bc.loc[i,\"percent_match\"] = data_fig5.loc[(data_fig5[\"race\"] == i) & mask_f & mask_yesmatch & \\\n",
    "        mask_yessame,\"iid\"].count() / data_fig5.loc[(data_fig5[\"race\"] == i) & \\\n",
    "        mask_f & mask_yesmatch,\"iid\"].count() * 100\n",
    "\n",
    "\n",
    "# get impact on decision to give a like\n",
    "\n",
    "# initialise variable to store results\n",
    "data_fig5b = pd.DataFrame(np.zeros((2,3)), columns = [\"info\",\"same\",\"not same\"])\n",
    "data_fig5b[\"info\"] = [\"No Like\", \"Like\"]\n",
    "\n",
    "# set masks\n",
    "mask_yeslike = data_fig5[\"dec\"] == \"Yes\"\n",
    "mask_nolike = data_fig5[\"dec\"] == \"No\"\n",
    "mask_yessame = data_fig5[\"samerace\"] == \"Yes\"\n",
    "mask_nosame = data_fig5[\"samerace\"] == \"No\"\n",
    "\n",
    "# fill dataframe with percentages\n",
    "data_fig5b.loc[0,\"same\"] = data_fig5.loc[mask_nolike & mask_yessame,\"iid\"].count() / \\\n",
    "    data_fig5.loc[mask_nolike,\"iid\"].count() * 100\n",
    "data_fig5b.loc[1,\"same\"] = data_fig5.loc[mask_yeslike & mask_yessame,\"iid\"].count() / \\\n",
    "    data_fig5.loc[mask_yeslike,\"iid\"].count() * 100\n",
    "data_fig5b.loc[0,\"not same\"] = data_fig5.loc[mask_nolike & mask_nosame,\"iid\"].count() / \\\n",
    "    data_fig5.loc[mask_nolike,\"iid\"].count() * 100\n",
    "data_fig5b.loc[1,\"not same\"] = data_fig5.loc[mask_yeslike & mask_nosame,\"iid\"].count() / \\\n",
    "    data_fig5.loc[mask_yeslike,\"iid\"].count() * 100\n",
    "\n",
    "\n",
    "# get impact on matches\n",
    "\n",
    "# copy data for safety\n",
    "data_fig5c = data_fig5.loc[:,[\"iid\", \"pid\", \"gender\", \"match\", \"samerace\"]].reset_index(drop = True)\n",
    "\n",
    "# initialise variable to store index of data to drop\n",
    "index_drop = []\n",
    "\n",
    "# identify reciprocal speed dates\n",
    "for i in range(0,data_fig5c.shape[0]):\n",
    "\n",
    "    # get subject and partner ids\n",
    "    # do it only for females, to not drop every date\n",
    "    if data_fig5c.loc[i,\"gender\"] == \"Female\":\n",
    "        subject_current = data_fig5c.loc[i,\"iid\"]\n",
    "        partner_current = data_fig5c.loc[i,\"pid\"]\n",
    "\n",
    "        # search for reciprocal speed date\n",
    "        mask_drop = (data_fig5c[\"iid\"] == partner_current) & (data_fig5c[\"pid\"] == subject_current)\n",
    "        index_drop.append(data_fig5c.loc[mask_drop,:].index[0])\n",
    "\n",
    "# drop reciprocal speed dates\n",
    "data_fig5c = data_fig5c.drop(index_drop, axis = 0)\n",
    "\n",
    "# initialise varibale to store results\n",
    "data_fig5cplot = pd.DataFrame(np.zeros((2,3)), columns = [\"info\",\"same\",\"not same\"])\n",
    "data_fig5cplot[\"info\"] = [\"No Match\", \"Match\"]\n",
    "\n",
    "# set masks\n",
    "mask_yeslike = data_fig5c[\"match\"] == \"Yes\"\n",
    "mask_nolike = data_fig5c[\"match\"] == \"No\"\n",
    "mask_yessame = data_fig5c[\"samerace\"] == \"Yes\"\n",
    "mask_nosame = data_fig5c[\"samerace\"] == \"No\"\n",
    "\n",
    "# fill dataframe with percentages\n",
    "data_fig5cplot.loc[0,\"same\"] = data_fig5c.loc[mask_nolike & mask_yessame,\"iid\"].count() / \\\n",
    "    data_fig5c.loc[mask_nolike,\"iid\"].count() * 100\n",
    "data_fig5cplot.loc[1,\"same\"] = data_fig5c.loc[mask_yeslike & mask_yessame,\"iid\"].count() / \\\n",
    "    data_fig5c.loc[mask_yeslike,\"iid\"].count() * 100\n",
    "data_fig5cplot.loc[0,\"not same\"] = data_fig5c.loc[mask_nolike & mask_nosame,\"iid\"].count() / \\\n",
    "    data_fig5c.loc[mask_nolike,\"iid\"].count() * 100\n",
    "data_fig5cplot.loc[1,\"not same\"] = data_fig5c.loc[mask_yeslike & mask_nosame,\"iid\"].count() / \\\n",
    "    data_fig5c.loc[mask_yeslike,\"iid\"].count() * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 8 - analysis 3 - importance and impact of shared ethnicity - plot ### ----\n",
    "\n",
    "# set figure to make subplots\n",
    "fig5 = make_subplots(\n",
    "    rows = 1,\n",
    "    cols = 3,\n",
    "    subplot_titles = (\n",
    "        \"A. Importance by ethnicity\",\n",
    "        \"B. Impact on decision\",\n",
    "        \"C. Impact on matches\"),\n",
    "    column_widths = [0.25, 0.25, 0.25],\n",
    "    horizontal_spacing = 0.15)\n",
    "\n",
    "# plot importance per ethnicity\n",
    "[fig5.add_trace(go.Box(\n",
    "        y = data_fig5.loc[data_fig5[\"race\"] == race_imp.index[i],\"imprace\"], \n",
    "        name = race_imp.index[i], \n",
    "        marker_color = px.colors.qualitative.Vivid[i+2],\n",
    "        showlegend = False),\n",
    "        row = 1, col = 1) for i in range(0,len(race_imp.index))]\n",
    "\n",
    "# plot impact of shared ethnicity on decision to give a like\n",
    "fig5.add_trace(go.Bar(\n",
    "    x = data_fig5bc.index,\n",
    "    y = data_fig5bc[\"percent_like\"],\n",
    "    marker_color = px.colors.qualitative.Vivid[2:],\n",
    "    showlegend = False),\n",
    "    row = 1, col = 2)\n",
    "\n",
    "# plot impact of shared ethnicity on matches\n",
    "fig5.add_trace(go.Bar(\n",
    "    x = data_fig5bc.index,\n",
    "    y = data_fig5bc[\"percent_match\"],\n",
    "    marker_color = px.colors.qualitative.Vivid[2:],\n",
    "    showlegend = False),\n",
    "    row = 1, col = 3)\n",
    "\n",
    "# update layout\n",
    "fig5.update_annotations(font_size = 15)\n",
    "fig5.update_xaxes(title_font = dict(size = 13), tickfont = dict(size = 10), tickangle = 90)\n",
    "fig5.update_yaxes(title_font = dict(size = 13), tickfont = dict(size = 10))\n",
    "fig5.update_layout(\n",
    "        margin = dict(l = 90),\n",
    "        title_text = \"Figure 5. Importance and impact of shared ethnicity\",\n",
    "        title_x = 0.5,\n",
    "        title_y = 0.95,\n",
    "        title_font_size = 18,\n",
    "        yaxis = dict(title = \"Rating (scale 1-10)\", range = [-1, 12], tickvals = [0, 2, 4, 6, 8, 10]),\n",
    "        yaxis2 = dict(title = \"Percent of total likes<br>per ethnicity\", range = [-6, 95], tickvals = [0, 20, 40, 60, 80]),\n",
    "        yaxis3 = dict(title = \"Percent of total matches<br>per ethnicity\", range = [-6, 95], tickvals = [0, 20, 40, 60, 80]),\n",
    "        legend = dict(\n",
    "            orientation = \"h\",\n",
    "            yanchor = \"top\",\n",
    "            y = -0.15,\n",
    "            xanchor = \"left\",\n",
    "            x = 0.46,\n",
    "            font = dict(size = 11)),\n",
    "        plot_bgcolor = \"rgba(0,0,0,0)\",\n",
    "        paper_bgcolor = \"rgb(232,232,232)\",\n",
    "        width = 800,\n",
    "        height = 400)\n",
    "\n",
    "fig5.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\n",
    "### 9 - Analysis 4 - Importance and impact of self-esteem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 9 - Analysis 4 - Importance and impact of self-esteem - get data ### ----\n",
    "\n",
    "# self-esteem is quantified as the mean of the differences between the mean of the grades given by the speed \n",
    "# date partners and the grades given by the subject for the same series of attributes\n",
    "\n",
    "# keep only columns that are relevant for the analysis\n",
    "columns_useful = [\"iid\", \"gender\", \"dec_o\", \"match\", \"attr3_1\", \"sinc3_1\", \"intel3_1\", \"fun3_1\", \"amb3_1\", \n",
    "    \"attr_o\", \"sinc_o\", \"intel_o\", \"fun_o\", \"amb_o\"]\n",
    "data_fig6 = data6[columns_useful]\n",
    "\n",
    "# get unique subject ids\n",
    "id_unique = data_fig6[\"iid\"].unique()\n",
    "\n",
    "# initialise new dataframe to store self-esteem measure by unique subject\n",
    "data_fig6a = pd.DataFrame(index = range(0,len(id_unique)), columns = [\"iid\",\"gender\", \"self_esteem\"])\n",
    "data_fig6a[\"iid\"] = id_unique\n",
    "\n",
    "# get a measure of sel-esteem\n",
    "for i in id_unique:\n",
    "\n",
    "    # get data\n",
    "    data_current = data_fig6.loc[data_fig6[\"iid\"] == i, :].reset_index(drop = True)\n",
    "\n",
    "    # get subject grades\n",
    "    subject_grades = data_current.loc[0,[\"attr3_1\", \"sinc3_1\", \"intel3_1\", \"fun3_1\", \"amb3_1\"]]\n",
    "\n",
    "    # get mean of grades given by partner\n",
    "    partner_grades = data_current.loc[:,[\"attr_o\", \"sinc_o\", \"intel_o\", \"fun_o\", \"amb_o\"]].mean()\n",
    "    \n",
    "    # get gender\n",
    "    data_fig6a.loc[data_fig6a[\"iid\"] == i,\"gender\"] = data_current.loc[0,\"gender\"]\n",
    "\n",
    "    # get score for self-esteem if no NaNs in data\n",
    "    if subject_grades.isnull().any() | partner_grades.isnull().any():\n",
    "        data_fig6a.loc[data_fig6a[\"iid\"] == i,\"self_esteem\"] = np.nan\n",
    "    else:\n",
    "        data_fig6a.loc[data_fig6a[\"iid\"] == i,\"self_esteem\"] = \\\n",
    "            [(partner_grades.values - subject_grades.values).mean()]\n",
    "\n",
    "# copy data for safety\n",
    "data_fig6b = data_fig6.copy()\n",
    "\n",
    "# update data with self-esteem measure\n",
    "for i in id_unique:\n",
    "    data_fig6b.loc[data_fig6b[\"iid\"] == i,\"self_esteem\"] = \\\n",
    "        [data_fig6a.loc[data_fig6a[\"iid\"] == i,\"self_esteem\"].values] * \\\n",
    "        data_fig6b.loc[data_fig6b[\"iid\"] == i,:].shape[0]\n",
    "    \n",
    "# set masks for likes given to partner and matches\n",
    "mask_yeslike = data_fig6b[\"dec_o\"] == \"Yes\"\n",
    "mask_nolike = data_fig6b[\"dec_o\"] == \"No\"\n",
    "mask_yesmatch = data_fig6b[\"match\"] == \"Yes\"\n",
    "mask_nomatch = data_fig6b[\"match\"] == \"No\"\n",
    "\n",
    "# assess the impact of self-esteem on receiving a like\n",
    "_, pvalue_like = stats.ttest_ind(\n",
    "    data_fig6b.loc[mask_yeslike,\"self_esteem\"], data_fig6b.loc[mask_nolike,\"self_esteem\"],\n",
    "    equal_var = False, nan_policy = \"omit\", alternative = \"two-sided\")\n",
    "\n",
    "# assess the impact of self-esteem on getting a match\n",
    "_, pvalue_match = stats.ttest_ind(\n",
    "    data_fig6b.loc[mask_yesmatch,\"self_esteem\"], data_fig6b.loc[mask_nomatch,\"self_esteem\"],\n",
    "    equal_var = False, nan_policy = \"omit\", alternative = \"two-sided\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 9 - Analysis 4 - Importance and impact of self-esteem - plot ### ----\n",
    "\n",
    "# set figure to make subplots\n",
    "fig6 = make_subplots(\n",
    "    rows = 1,\n",
    "    cols = 3,\n",
    "    subplot_titles = (\n",
    "        \"A. Self-esteem distribution<br> \",\n",
    "        \"B. Impact on like from partner<br>(pvalue = {:.4f})\".format(pvalue_like),\n",
    "        \"C. Impact on matches<br>(pvalue = {:.4f})\".format(pvalue_match, \".4f\")),\n",
    "    column_widths = [0.25, 0.25, 0.25],\n",
    "    horizontal_spacing = 0.15)\n",
    "\n",
    "# plot distribution of self-esteem per gender\n",
    "fig6.add_trace(go.Histogram(\n",
    "    x = data_fig6a.loc[data_fig6a[\"gender\"] == \"Female\",\"self_esteem\"],\n",
    "    name = \"Females\",\n",
    "    opacity = 0.6),\n",
    "    row = 1, col = 1)\n",
    "fig6.add_trace(go.Histogram(\n",
    "    x = data_fig6a.loc[data_fig6a[\"gender\"] == \"Male\",\"self_esteem\"],\n",
    "    name = \"Males\",\n",
    "    opacity = 0.6),\n",
    "    row = 1, col = 1)\n",
    "\n",
    "# plot impact on decision to give a like\n",
    "fig6.add_trace(go.Box(\n",
    "        y = data_fig6b.loc[mask_nolike,\"self_esteem\"],\n",
    "        name = \"No Like\",\n",
    "        marker_color = px.colors.qualitative.Vivid[4],\n",
    "        showlegend = False),\n",
    "        row = 1, col = 2)\n",
    "fig6.add_trace(go.Box(\n",
    "        y = data_fig6b.loc[mask_yeslike,\"self_esteem\"],\n",
    "        name = \"Like\",\n",
    "        marker_color = px.colors.qualitative.Vivid[4],\n",
    "        showlegend = False),\n",
    "        row = 1, col = 2)\n",
    "\n",
    "# plot impact on matches\n",
    "fig6.add_trace(go.Box(\n",
    "        y = data_fig6b.loc[mask_nomatch,\"self_esteem\"],\n",
    "        name = \"No Match\",\n",
    "        marker_color = px.colors.qualitative.Vivid[5],\n",
    "        showlegend = False),\n",
    "        row = 1, col = 3)\n",
    "fig6.add_trace(go.Box(\n",
    "        y = data_fig6b.loc[mask_yesmatch,\"self_esteem\"],\n",
    "        name = \"Match\",\n",
    "        marker_color = px.colors.qualitative.Vivid[5],\n",
    "        showlegend = False),\n",
    "        row = 1, col = 3)\n",
    "\n",
    "# update layout\n",
    "fig6.update_annotations(font_size = 15)\n",
    "fig6.update_xaxes(title_font = dict(size = 13), tickfont = dict(size = 10))\n",
    "fig6.update_yaxes(title_font = dict(size = 13), tickfont = dict(size = 10))\n",
    "fig6.update_layout(\n",
    "        margin = dict(l = 90),\n",
    "        title_text = \"Figure 6. Importance and impact of self_esteem\",\n",
    "        title_x = 0.5,\n",
    "        title_y = 0.95,\n",
    "        title_font_size = 18,\n",
    "        barmode = \"overlay\",\n",
    "        xaxis = dict(title = \"Self-esteem\", range = [-6, 4], tickvals = [-4, -2, 0, 2]),\n",
    "        yaxis = dict(range = [-1, 18], tickvals = [0, 5, 10, 15]),\n",
    "        yaxis2 = dict(title = \"Self-esteem\", range = [-5.3, 3.4], tickvals = [-5, -4, -3, -2, -1, 0, 1, 2]),\n",
    "        yaxis3 = dict(title = \"Self-esteem\", range = [-5.3, 3.4], tickvals = [-5, -4, -3, -2, -1, 0, 1, 2]),\n",
    "        legend = dict(\n",
    "            orientation = \"h\",\n",
    "            yanchor = \"top\",\n",
    "            y = 1.11,\n",
    "            xanchor = \"left\",\n",
    "            x = -0.025,\n",
    "            font = dict(size = 11)),\n",
    "        plot_bgcolor = \"rgba(0,0,0,0)\",\n",
    "        paper_bgcolor = \"rgb(232,232,232)\",\n",
    "        width = 800,\n",
    "        height = 400)\n",
    "\n",
    "fig6.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\n",
    "### 10 - Analysis 5 - Importance and impact of the dating goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 10 - Analysis 5 - Importance and impact of the dating goal - get data ### ----\n",
    "\n",
    "# people have different goals or expectations when signing up on a dating app\n",
    "\n",
    "# compare goals of females and males\n",
    "# assess the impact of goals on the decision to give a like and on matches\n",
    "\n",
    "# keep only columns that are relevant for the analysis\n",
    "columns_useful = [\"iid\", \"pid\", \"gender\", \"dec\", \"dec_o\", \"match\", \"goal\"]\n",
    "data_fig7 = data6[columns_useful].reset_index(drop = True)\n",
    "\n",
    "# set masks for genders\n",
    "mask_f = data_fig7[\"gender\"] == \"Female\"\n",
    "mask_m = data_fig7[\"gender\"] == \"Male\"\n",
    "\n",
    "# get unique goals\n",
    "goals_unique = data_fig7[\"goal\"].dropna().unique()\n",
    "\n",
    "# initialise dataframe to store results\n",
    "data_fig7ab_f = pd.DataFrame(index = goals_unique, columns = [\"count\", \"percent_like_g\", \"percent_match\"])\n",
    "data_fig7ab_m = pd.DataFrame(index = goals_unique, columns = [\"count\", \"percent_like_g\", \"percent_match\"])\n",
    "\n",
    "# fill dataframe with results\n",
    "for i in goals_unique:\n",
    "\n",
    "    # goal count per gender\n",
    "    data_fig7ab_f.loc[i,\"count\"] = data_fig7.loc[mask_f & (data_fig7[\"goal\"] == i),\"goal\"].count()\n",
    "    data_fig7ab_m.loc[i,\"count\"] = data_fig7.loc[mask_m & (data_fig7[\"goal\"] == i),\"goal\"].count()\n",
    "\n",
    "    # percent like given per goal per gender\n",
    "    data_fig7ab_f.loc[i,\"percent_like_g\"] = data_fig7.loc[mask_f & (data_fig7[\"goal\"] == i) & \\\n",
    "        (data_fig7[\"dec\"] == \"Yes\"),\"goal\"].count() / data_fig7ab_f.loc[i,\"count\"] * 100\n",
    "    data_fig7ab_m.loc[i,\"percent_like_g\"] = data_fig7.loc[mask_m & (data_fig7[\"goal\"] == i) & \\\n",
    "        (data_fig7[\"dec\"] == \"Yes\"),\"goal\"].count() / data_fig7ab_m.loc[i,\"count\"] * 100\n",
    "\n",
    "    # percent match per goal per gender\n",
    "    data_fig7ab_f.loc[i,\"percent_match\"] = data_fig7.loc[mask_f & (data_fig7[\"goal\"] == i) & \\\n",
    "        (data_fig7[\"match\"] == \"Yes\"),\"goal\"].count() / data_fig7ab_f.loc[i,\"count\"] * 100\n",
    "    data_fig7ab_m.loc[i,\"percent_match\"] = data_fig7.loc[mask_m & (data_fig7[\"goal\"] == i) & \\\n",
    "        (data_fig7[\"match\"] == \"Yes\"),\"goal\"].count() / data_fig7ab_m.loc[i,\"count\"] * 100\n",
    "\n",
    "# sort data by count\n",
    "data_fig7ab_f = data_fig7ab_f.sort_values(\"count\", ascending = False)\n",
    "data_fig7ab_m = data_fig7ab_m.sort_values(\"count\", ascending = False)\n",
    "\n",
    "# create a new column \"same_goal\" to record goal matching between partners\n",
    "for i in range(0,data_fig7.shape[0]):\n",
    "\n",
    "    # get id of partner\n",
    "    id_partner = data_fig7.loc[i,\"pid\"]\n",
    "\n",
    "    # get goal of subject and partner\n",
    "    goal_subject = data_fig7.loc[i,\"goal\"]\n",
    "    goal_partner = data_fig7.loc[data_fig7[\"iid\"] == id_partner,\"goal\"].reset_index(drop = True)\n",
    "    goal_partner = goal_partner[0]\n",
    "\n",
    "    # record goal matching\n",
    "    if goal_subject == goal_partner:\n",
    "        data_fig7.loc[i,\"same_goal\"] = \"Yes\"\n",
    "    else:\n",
    "        data_fig7.loc[i,\"same_goal\"] = \"No\"\n",
    "\n",
    "\n",
    "# get impact of having the same goal on decision to give a like and on getting a match\n",
    "\n",
    "# initialise variables to store results on impact\n",
    "data_fig7c_f = pd.DataFrame(index = data_fig7ab_f.index, columns = [\"percent_like\", \"percent_match\"])\n",
    "data_fig7c_m = pd.DataFrame(index = data_fig7ab_m.index, columns = [\"percent_like\", \"percent_match\"])\n",
    "\n",
    "# set masks\n",
    "mask_yeslike = data_fig7[\"dec\"] == \"Yes\"\n",
    "mask_yesmatch = data_fig7[\"match\"] == \"Yes\"\n",
    "mask_yessame = data_fig7[\"same_goal\"] == \"Yes\"\n",
    "\n",
    "# fill dataframe with percentages\n",
    "for i in goals_unique:\n",
    "\n",
    "    # for likes\n",
    "    data_fig7c_f.loc[i,\"percent_like\"] = data_fig7.loc[(data_fig7[\"goal\"] == i) & mask_f & mask_yeslike & \\\n",
    "        mask_yessame,\"iid\"].count() / data_fig7.loc[(data_fig7[\"goal\"] == i) & mask_f & mask_yeslike,\"iid\"].count() * 100\n",
    "    data_fig7c_m.loc[i,\"percent_like\"] = data_fig7.loc[(data_fig7[\"goal\"] == i) & mask_m & mask_yeslike & \\\n",
    "        mask_yessame,\"iid\"].count() / data_fig7.loc[(data_fig7[\"goal\"] == i) & mask_m & mask_yeslike,\"iid\"].count() * 100\n",
    "    \n",
    "    # for matches (only for females to drop reciprocal dates)\n",
    "    data_fig7c_f.loc[i,\"percent_match\"] = data_fig7.loc[(data_fig7[\"goal\"] == i) & mask_f & mask_yesmatch & \\\n",
    "        mask_yessame,\"iid\"].count() / data_fig7.loc[(data_fig7[\"goal\"] == i) & mask_f & mask_yesmatch,\"iid\"].count() * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 10 - Analysis 5 - Importance and impact of the dating goal - plot ### ----\n",
    "\n",
    "# set figure to make subplots\n",
    "fig7 = make_subplots(\n",
    "    rows = 5,\n",
    "    cols = 4,\n",
    "    specs = [[{\"colspan\": 2}, None, {\"colspan\": 2}, None], [{\"colspan\": 2}, None, {\"colspan\": 2}, None],\n",
    "        [{\"colspan\": 2}, None, {\"colspan\": 2}, None], [{\"colspan\": 2}, None, {\"colspan\": 2}, None],\n",
    "        [None, {\"colspan\": 2}, None, None]],\n",
    "    subplot_titles = (\n",
    "        \"A. Goals for females\",\n",
    "        \"B. Goals for males\",\n",
    "        \"C. Proportion of likes given by females\",\n",
    "        \"D. Proportion of likes given by males\",\n",
    "        \"E. Proportion of matches for females\",\n",
    "        \"F. Proportion of matches for males\",\n",
    "        \"G. Proportion of likes given by females<br>to males having the same goal\",\n",
    "        \"H. Proportion of likes given by males<br>to females having the same goal\",\n",
    "        \"I. Impact of shared goals on matches\"),\n",
    "    column_widths = [0.2, 0.2, 0.2, 0.2],\n",
    "    horizontal_spacing = 0.15)\n",
    "\n",
    "# plot importance of goals\n",
    "fig7.add_trace(go.Bar(\n",
    "    x = data_fig7ab_f.index,\n",
    "    y = data_fig7ab_f[\"count\"],\n",
    "    marker_color = px.colors.qualitative.Vivid[1]),\n",
    "    row = 1, col = 1)\n",
    "fig7.add_trace(go.Bar(\n",
    "    x = data_fig7ab_m.index,\n",
    "    y = data_fig7ab_m[\"count\"],\n",
    "    marker_color = px.colors.qualitative.Vivid[0]),\n",
    "    row = 1, col = 3)\n",
    "\n",
    "# plot percent of likes given per goal\n",
    "fig7.add_trace(go.Bar(\n",
    "    x = data_fig7ab_f.index,\n",
    "    y = data_fig7ab_f[\"percent_like_g\"],\n",
    "    marker_color = px.colors.qualitative.Vivid[1]),\n",
    "    row = 2, col = 1)\n",
    "fig7.add_trace(go.Bar(\n",
    "    x = data_fig7ab_m.index,\n",
    "    y = data_fig7ab_m[\"percent_like_g\"],\n",
    "    marker_color = px.colors.qualitative.Vivid[0]),\n",
    "    row = 2, col = 3)\n",
    "\n",
    "# plot percent of matches per goal\n",
    "fig7.add_trace(go.Bar(\n",
    "    x = data_fig7ab_f.index,\n",
    "    y = data_fig7ab_f[\"percent_match\"],\n",
    "    marker_color = px.colors.qualitative.Vivid[1]),\n",
    "    row = 3, col = 1)\n",
    "fig7.add_trace(go.Bar(\n",
    "    x = data_fig7ab_m.index,\n",
    "    y = data_fig7ab_m[\"percent_match\"],\n",
    "    marker_color = px.colors.qualitative.Vivid[0]),\n",
    "    row = 3, col = 3)\n",
    "\n",
    "# plot impact of shared goals on decision to give a like\n",
    "fig7.add_trace(go.Bar(\n",
    "    x = data_fig7c_f.index,\n",
    "    y = data_fig7c_f[\"percent_like\"],\n",
    "    marker_color = px.colors.qualitative.Vivid[1]),\n",
    "    row = 4, col = 1)\n",
    "fig7.add_trace(go.Bar(\n",
    "    x = data_fig7c_m.index,\n",
    "    y = data_fig7c_m[\"percent_like\"],\n",
    "    marker_color = px.colors.qualitative.Vivid[0]),\n",
    "    row = 4, col = 3)\n",
    "\n",
    "# plot impact of shared goals on matches\n",
    "fig7.add_trace(go.Bar(\n",
    "    x = data_fig7c_f.sort_values(by = \"percent_match\", ascending = False).index,\n",
    "    y = data_fig7c_f[\"percent_match\"].sort_values(ascending = False),\n",
    "    marker_color = px.colors.qualitative.Vivid[3]),\n",
    "    row = 5, col = 2)\n",
    "\n",
    "# update layout\n",
    "fig7.update_annotations(font_size = 15)\n",
    "fig7.update_xaxes(title_font = dict(size = 13), tickfont = dict(size = 10), tickangle = 90)\n",
    "fig7.update_yaxes(title_font = dict(size = 13), tickfont = dict(size = 10))\n",
    "fig7.update_layout(\n",
    "        margin = dict(l = 90, t = 120),\n",
    "        title_text = \"Figure 7. Importance and impact of dating goals\",\n",
    "        title_x = 0.5,\n",
    "        title_y = 0.98,\n",
    "        title_font_size = 18,\n",
    "        yaxis = dict(title = \"Subject count\", range = [-20, 500], tickvals = [0, 100, 200, 300, 400]),\n",
    "        yaxis2 = dict(title = \"Subject count\", range = [-20, 500], tickvals = [0, 100, 200, 300, 400]),\n",
    "        yaxis3 = dict(title = \"Percent likes per goal\", range = [-10, 120], tickvals = [0, 20, 40, 60, 80, 100]),\n",
    "        yaxis4 = dict(title = \"Percent likes per goal\", range = [-10, 120], tickvals = [0, 20, 40, 60, 80, 100]),\n",
    "        yaxis5 = dict(title = \"Percent matches per goal\", range = [-10, 120], tickvals = [0, 20, 40, 60, 80, 100]),\n",
    "        yaxis6 = dict(title = \"Percent matches per goal\", range = [-10, 120], tickvals = [0, 20, 40, 60, 80, 100]),\n",
    "        yaxis7 = dict(title = \"Percent of total likes<br>per goal\", range = [-6, 72], tickvals = [0, 20, 40, 60]),\n",
    "        yaxis8 = dict(title = \"Percent of total likes<br>per goal\", range = [-6, 72], tickvals = [0, 20, 40, 60]),\n",
    "        yaxis9 = dict(title = \"Percent of total matches<br>per goal\", range = [-6, 72], tickvals = [0, 20, 40, 60]),\n",
    "        legend = dict(\n",
    "            orientation = \"h\",\n",
    "            yanchor = \"top\",\n",
    "            y = -0.02,\n",
    "            xanchor = \"left\",\n",
    "            x = 0.3,\n",
    "            font = dict(size = 11)),\n",
    "        showlegend = False,\n",
    "        plot_bgcolor = \"rgba(0,0,0,0)\",\n",
    "        paper_bgcolor = \"rgb(232,232,232)\",\n",
    "        width = 800,\n",
    "        height = 1600)\n",
    "\n",
    "fig7.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6274bd53ad02975ad3a1fc7c53468a83b5e714d6ade680c552f31a0436ec5d39"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
